{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import multiprocessing\n",
    "import logging\n",
    "\n",
    "from gensim.models.fasttext import FastText\n",
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "from utils.utils import LineSentences\n",
    "from utils.utils import callback"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gensim FastText\n",
    "Documentation: https://radimrehurek.com/gensim/models/fasttext.html\n",
    "* `sentences` _(iterable of iterables, optional)_: The sentences iterable can be simply a list of lists of tokens, but for larger corpora, consider an iterable that streams the sentences directly from disk/network. See `BrownCorpus`, `Text8Corpus` or `LineSentence` in word2vec module for such examples. See also the tutorial on data streaming in Python. If you don’t supply sentences, the model is left uninitialized – use if you plan to initialize it in some other way.\n",
    "* `vector_size` _(int, optional)_: Dimensionality of the word vectors.\n",
    "* `window` _(int, optional)_: Maximum distance between the current and predicted word within a sentence.\n",
    "* `min_count` _(int, optional)_: Ignores all words with total frequency lower than this.\n",
    "* `workers` _(int, optional)_: Use these many worker threads to train the model (=faster training with multicore machines).\n",
    "* `sg` _({0, 1}, optional)_: Training algorithm: 1 for skip-gram; otherwise CBOW.\n",
    "* `hs` _({0, 1}, optional)_: If 1, hierarchical softmax will be used for model training. If 0, and negative is non-zero, negative sampling will be used.\n",
    "* `negative` _(int, optional)_: If > 0, negative sampling will be used, the int for negative specifies how many “noise words” should be drawn (usually between 5-20). If set to 0, no negative sampling is used.\n",
    "* `min_n` (int, optional) – Min length of char ngrams to be used for training word representations.\n",
    "* `max_n` (int, optional) – Max length of char ngrams to be used for training word representations.\n",
    "* `word_ngrams` _(int, optional)_: If 1, uses enriches word vectors with subword(n-gram) information. If 0, this is equivalent to Word2Vec. If > 1, this parameter is ignored and subwords are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documentation: https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec\n",
    "# Assumption: Provided input is a txt file with one sentence per line.\n",
    "INPUT = [\"../corpus/turkish-texts-tokenized.txt\", \"../corpus/bounwebcorpus.txt\"]\n",
    "MIN_COUNT = 10   # ignore all words with total frequency lower than this\n",
    "EMB = 300        # dimensionality of word vectors\n",
    "WINDOW = 5       # maximum distance between the target and context word within a sentence\n",
    "EPOCH = 5        # number of iterations (epochs) over the corpus\n",
    "SG = 1           # training algorithm: 1 for skip-gram; otherwise CBOW\n",
    "HS = 0           # if 1, hierarchical softmax will be used for model training. If set to 0, and negative is non-zero, negative sampling will be used. If both of them 0, no training algorithm will be used.\n",
    "NEGATIVE = 5     # if > 0, negative sampling will be used, the int for negative specifies how many \"noise words\" should be drawn (usually between 5-20). If set to 0, no negative sampling is used.\n",
    "MINN = 3         # min length of char ngram\n",
    "MAXN = 6         # max length of char ngram\n",
    "WNG = 1          # In Facebook’s FastText, “max length of word ngram” - but gensim only supports the default of 1 (regular unigram word handling).\n",
    "OUTPUT = \"fasttext-5epoch.wordvectors\"\n",
    "\n",
    "# So, if both hs and negative are set to 0, it means that no training algorithm will be used to learn the word embeddings. In this case, you will have to provide pre-trained word embeddings for the model to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "model = FastText(vector_size=EMB, \n",
    "                window=WINDOW, \n",
    "                min_count=MIN_COUNT, \n",
    "                sg = SG,\n",
    "                hs = HS,\n",
    "                negative = NEGATIVE,\n",
    "                min_n=MINN,\n",
    "                max_n=MAXN,\n",
    "                word_ngrams=WNG,\n",
    "                workers=multiprocessing.cpu_count(),\n",
    "                callbacks=[callback()])\n",
    "\n",
    "model.build_vocab(corpus_iterable=LineSentences(INPUT))\n",
    "model.train(corpus_iterable=LineSentences(INPUT), epochs = model.epochs, total_examples=model.corpus_count, compute_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.save_word2vec_format(OUTPUT, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = FastText.load(OUTPUT).wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors.most_similar_cosmul(positive=['kadın', 'kral'], negative=['adam'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
