{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import multiprocessing\n",
    "import logging\n",
    "\n",
    "from gensim.models.fasttext import FastText\n",
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "from utils.utils import LineSentences\n",
    "from utils.utils import callback"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gensim FastText\n",
    "Documentation: https://radimrehurek.com/gensim/models/fasttext.html\n",
    "* `sentences` _(iterable of iterables, optional)_: The sentences iterable can be simply a list of lists of tokens, but for larger corpora, consider an iterable that streams the sentences directly from disk/network. See `BrownCorpus`, `Text8Corpus` or `LineSentence` in word2vec module for such examples. See also the tutorial on data streaming in Python. If you don’t supply sentences, the model is left uninitialized – use if you plan to initialize it in some other way.\n",
    "* `vector_size` _(int, optional)_: Dimensionality of the word vectors.\n",
    "* `window` _(int, optional)_: Maximum distance between the current and predicted word within a sentence.\n",
    "* `min_count` _(int, optional)_: Ignores all words with total frequency lower than this.\n",
    "* `workers` _(int, optional)_: Use these many worker threads to train the model (=faster training with multicore machines).\n",
    "* `sg` _({0, 1}, optional)_: Training algorithm: 1 for skip-gram; otherwise CBOW.\n",
    "* `hs` _({0, 1}, optional)_: If 1, hierarchical softmax will be used for model training. If 0, and negative is non-zero, negative sampling will be used.\n",
    "* `negative` _(int, optional)_: If > 0, negative sampling will be used, the int for negative specifies how many “noise words” should be drawn (usually between 5-20). If set to 0, no negative sampling is used.\n",
    "* `min_n` (int, optional) – Min length of char ngrams to be used for training word representations.\n",
    "* `max_n` (int, optional) – Max length of char ngrams to be used for training word representations.\n",
    "* `word_ngrams` _(int, optional)_: If 1, uses enriches word vectors with subword(n-gram) information. If 0, this is equivalent to Word2Vec. If > 1, this parameter is ignored and subwords are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documentation: https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec\n",
    "# Assumption: Provided input is a txt file with one sentence per line.\n",
    "INPUT = [\"../corpus/turkish-texts-tokenized.txt\", \"../corpus/bounwebcorpus.txt\"]\n",
    "OUTPUT = \"fasttext.model\"\n",
    "MIN_COUNT = 10   # ignore all words with total frequency lower than this\n",
    "EMB = 300        # dimensionality of word vectors\n",
    "WINDOW = 5       # maximum distance between the target and context word within a sentence\n",
    "EPOCH = 5        # number of iterations (epochs) over the corpus\n",
    "SG = 1           # training algorithm: 1 for skip-gram; otherwise CBOW\n",
    "HS = 0           # if 1, hierarchical softmax will be used for model training. If set to 0, and negative is non-zero, negative sampling will be used. If both of them 0, no training algorithm will be used.\n",
    "NEGATIVE = 5     # if > 0, negative sampling will be used, the int for negative specifies how many \"noise words\" should be drawn (usually between 5-20). If set to 0, no negative sampling is used.\n",
    "MINN = 3         # min length of char ngram\n",
    "MAXN = 6         # max length of char ngram\n",
    "WNG = 1          # In Facebook’s FastText, “max length of word ngram” - but gensim only supports the default of 1 (regular unigram word handling).\n",
    "OUTPUT = \"fasttext-5epoch.model\"\n",
    "\n",
    "# So, if both hs and negative are set to 0, it means that no training algorithm will be used to learn the word embeddings. In this case, you will have to provide pre-trained word embeddings for the model to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-12 17:09:08,003 : WARNING : Callbacks are no longer retained by the model, so must be provided whenever training is triggered, as in initialization with a corpus or calling `train()`. The callbacks provided in this initialization without triggering train will be ignored.\n",
      "2023-03-12 17:09:08,004 : INFO : FastText lifecycle event {'params': 'FastText<vocab=0, vector_size=300, alpha=0.025>', 'datetime': '2023-03-12T17:09:08.004987', 'gensim': '4.3.0', 'python': '3.9.2 (tags/v3.9.2:1a79785, Feb 19 2021, 13:44:55) [MSC v.1928 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'created'}\n",
      "2023-03-12 17:09:08,005 : INFO : collecting all words and their counts\n",
      "2023-03-12 17:09:08,006 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2023-03-12 17:09:08,098 : INFO : PROGRESS: at sentence #10000, processed 140967 words, keeping 31034 word types\n",
      "2023-03-12 17:09:08,188 : INFO : PROGRESS: at sentence #20000, processed 291342 words, keeping 48763 word types\n",
      "2023-03-12 17:09:08,266 : INFO : PROGRESS: at sentence #30000, processed 445293 words, keeping 61864 word types\n",
      "2023-03-12 17:09:08,344 : INFO : PROGRESS: at sentence #40000, processed 594077 words, keeping 73545 word types\n",
      "2023-03-12 17:09:08,421 : INFO : PROGRESS: at sentence #50000, processed 739282 words, keeping 83337 word types\n",
      "2023-03-12 17:09:08,515 : INFO : PROGRESS: at sentence #60000, processed 877677 words, keeping 92131 word types\n",
      "2023-03-12 17:09:08,635 : INFO : PROGRESS: at sentence #70000, processed 1021906 words, keeping 100356 word types\n",
      "2023-03-12 17:09:08,753 : INFO : PROGRESS: at sentence #80000, processed 1164328 words, keeping 108525 word types\n",
      "2023-03-12 17:09:08,837 : INFO : PROGRESS: at sentence #90000, processed 1312344 words, keeping 115740 word types\n",
      "2023-03-12 17:09:08,918 : INFO : PROGRESS: at sentence #100000, processed 1452479 words, keeping 122405 word types\n",
      "2023-03-12 17:09:09,008 : INFO : PROGRESS: at sentence #110000, processed 1596576 words, keeping 129328 word types\n",
      "2023-03-12 17:09:09,113 : INFO : PROGRESS: at sentence #120000, processed 1734054 words, keeping 135490 word types\n",
      "2023-03-12 17:09:09,212 : INFO : PROGRESS: at sentence #130000, processed 1879499 words, keeping 140961 word types\n",
      "2023-03-12 17:09:09,310 : INFO : PROGRESS: at sentence #140000, processed 2020082 words, keeping 146787 word types\n",
      "2023-03-12 17:09:09,414 : INFO : PROGRESS: at sentence #150000, processed 2162969 words, keeping 152515 word types\n",
      "2023-03-12 17:09:09,497 : INFO : PROGRESS: at sentence #160000, processed 2301121 words, keeping 158465 word types\n",
      "2023-03-12 17:09:09,569 : INFO : PROGRESS: at sentence #170000, processed 2441699 words, keeping 164297 word types\n",
      "2023-03-12 17:09:09,660 : INFO : PROGRESS: at sentence #180000, processed 2586240 words, keeping 169309 word types\n",
      "2023-03-12 17:09:09,764 : INFO : PROGRESS: at sentence #190000, processed 2732477 words, keeping 174414 word types\n",
      "2023-03-12 17:09:09,853 : INFO : PROGRESS: at sentence #200000, processed 2878547 words, keeping 179605 word types\n",
      "2023-03-12 17:09:09,931 : INFO : PROGRESS: at sentence #210000, processed 3029490 words, keeping 184847 word types\n",
      "2023-03-12 17:09:10,014 : INFO : PROGRESS: at sentence #220000, processed 3176304 words, keeping 190178 word types\n",
      "2023-03-12 17:09:10,090 : INFO : PROGRESS: at sentence #230000, processed 3326230 words, keeping 194889 word types\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 14\u001b[0m\n\u001b[0;32m      1\u001b[0m logging\u001b[39m.\u001b[39mbasicConfig(\u001b[39mformat\u001b[39m\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%(asctime)s\u001b[39;00m\u001b[39m : \u001b[39m\u001b[39m%(levelname)s\u001b[39;00m\u001b[39m : \u001b[39m\u001b[39m%(message)s\u001b[39;00m\u001b[39m'\u001b[39m, level\u001b[39m=\u001b[39mlogging\u001b[39m.\u001b[39mINFO)\n\u001b[0;32m      2\u001b[0m model \u001b[39m=\u001b[39m FastText(vector_size\u001b[39m=\u001b[39mEMB, \n\u001b[0;32m      3\u001b[0m                 window\u001b[39m=\u001b[39mWINDOW, \n\u001b[0;32m      4\u001b[0m                 min_count\u001b[39m=\u001b[39mMIN_COUNT, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m                 workers\u001b[39m=\u001b[39mmultiprocessing\u001b[39m.\u001b[39mcpu_count(),\n\u001b[0;32m     12\u001b[0m                 callbacks\u001b[39m=\u001b[39m[callback()])\n\u001b[1;32m---> 14\u001b[0m model\u001b[39m.\u001b[39;49mbuild_vocab(corpus_iterable\u001b[39m=\u001b[39;49mLineSentences(INPUT))\n\u001b[0;32m     15\u001b[0m model\u001b[39m.\u001b[39mtrain(corpus_iterable\u001b[39m=\u001b[39mLineSentences(INPUT), epochs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mepochs, total_examples\u001b[39m=\u001b[39mmodel\u001b[39m.\u001b[39mcorpus_count, compute_loss\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\karab\\Desktop\\turkish-word-embeddings\\env\\lib\\site-packages\\gensim\\models\\word2vec.py:491\u001b[0m, in \u001b[0;36mWord2Vec.build_vocab\u001b[1;34m(self, corpus_iterable, corpus_file, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)\u001b[0m\n\u001b[0;32m    453\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Build vocabulary from a sequence of sentences (can be a once-only generator stream).\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \n\u001b[0;32m    455\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    488\u001b[0m \n\u001b[0;32m    489\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    490\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_corpus_sanity(corpus_iterable\u001b[39m=\u001b[39mcorpus_iterable, corpus_file\u001b[39m=\u001b[39mcorpus_file, passes\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m--> 491\u001b[0m total_words, corpus_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscan_vocab(\n\u001b[0;32m    492\u001b[0m     corpus_iterable\u001b[39m=\u001b[39;49mcorpus_iterable, corpus_file\u001b[39m=\u001b[39;49mcorpus_file, progress_per\u001b[39m=\u001b[39;49mprogress_per, trim_rule\u001b[39m=\u001b[39;49mtrim_rule)\n\u001b[0;32m    493\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcorpus_count \u001b[39m=\u001b[39m corpus_count\n\u001b[0;32m    494\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcorpus_total_words \u001b[39m=\u001b[39m total_words\n",
      "File \u001b[1;32mc:\\Users\\karab\\Desktop\\turkish-word-embeddings\\env\\lib\\site-packages\\gensim\\models\\word2vec.py:586\u001b[0m, in \u001b[0;36mWord2Vec.scan_vocab\u001b[1;34m(self, corpus_iterable, corpus_file, progress_per, workers, trim_rule)\u001b[0m\n\u001b[0;32m    583\u001b[0m \u001b[39mif\u001b[39;00m corpus_file:\n\u001b[0;32m    584\u001b[0m     corpus_iterable \u001b[39m=\u001b[39m LineSentence(corpus_file)\n\u001b[1;32m--> 586\u001b[0m total_words, corpus_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_scan_vocab(corpus_iterable, progress_per, trim_rule)\n\u001b[0;32m    588\u001b[0m logger\u001b[39m.\u001b[39minfo(\n\u001b[0;32m    589\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcollected \u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m word types from a corpus of \u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m raw words and \u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m sentences\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    590\u001b[0m     \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw_vocab), total_words, corpus_count\n\u001b[0;32m    591\u001b[0m )\n\u001b[0;32m    593\u001b[0m \u001b[39mreturn\u001b[39;00m total_words, corpus_count\n",
      "File \u001b[1;32mc:\\Users\\karab\\Desktop\\turkish-word-embeddings\\env\\lib\\site-packages\\gensim\\models\\word2vec.py:555\u001b[0m, in \u001b[0;36mWord2Vec._scan_vocab\u001b[1;34m(self, sentences, progress_per, trim_rule)\u001b[0m\n\u001b[0;32m    553\u001b[0m vocab \u001b[39m=\u001b[39m defaultdict(\u001b[39mint\u001b[39m)\n\u001b[0;32m    554\u001b[0m checked_string_types \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m--> 555\u001b[0m \u001b[39mfor\u001b[39;00m sentence_no, sentence \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(sentences):\n\u001b[0;32m    556\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m checked_string_types:\n\u001b[0;32m    557\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(sentence, \u001b[39mstr\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\karab\\Desktop\\turkish-word-embeddings\\utils\\utils.py:10\u001b[0m, in \u001b[0;36mLineSentences.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__iter__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m      9\u001b[0m     \u001b[39mfor\u001b[39;00m filename \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfilenames:\n\u001b[1;32m---> 10\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m \u001b[39mopen\u001b[39m(filename, \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m, encoding\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     11\u001b[0m             \u001b[39myield\u001b[39;00m line\u001b[39m.\u001b[39mstrip()\u001b[39m.\u001b[39msplit()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\codecs.py:319\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_buffer_decode\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, errors, final):\n\u001b[0;32m    315\u001b[0m     \u001b[39m# Overwrite this method in subclasses: It must decode input\u001b[39;00m\n\u001b[0;32m    316\u001b[0m     \u001b[39m# and return an (output, length consumed) tuple\u001b[39;00m\n\u001b[0;32m    317\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m\n\u001b[1;32m--> 319\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, final\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    320\u001b[0m     \u001b[39m# decode input (taking the buffer into account)\u001b[39;00m\n\u001b[0;32m    321\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuffer \u001b[39m+\u001b[39m \u001b[39minput\u001b[39m\n\u001b[0;32m    322\u001b[0m     (result, consumed) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_buffer_decode(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39merrors, final)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "model = FastText(vector_size=EMB, \n",
    "                window=WINDOW, \n",
    "                min_count=MIN_COUNT, \n",
    "                sg = SG,\n",
    "                hs = HS,\n",
    "                negative = NEGATIVE,\n",
    "                min_n=MINN,\n",
    "                max_n=MAXN,\n",
    "                word_ngrams=WNG,\n",
    "                workers=multiprocessing.cpu_count(),\n",
    "                callbacks=[callback()])\n",
    "\n",
    "model.build_vocab(corpus_iterable=LineSentences(INPUT))\n",
    "model.train(corpus_iterable=LineSentences(INPUT), epochs = model.epochs, total_examples=model.corpus_count, compute_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "with tempfile.NamedTemporaryFile(prefix=OUTPUT, delete=False) as tmp:\n",
    "    model.save(tmp.name, separately=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = FastText.load(OUTPUT).wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors.most_similar_cosmul(positive=['kadın', 'kral'], negative=['adam'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
