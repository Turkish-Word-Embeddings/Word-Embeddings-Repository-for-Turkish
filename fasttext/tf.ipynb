{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://fasttext.cc/docs/en/unsupervised-tutorial.html\n",
    "# paper: https://arxiv.org/pdf/1607.04606.pdf\n",
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"skipgram\" # or \"cbow\"\n",
    "INPUT = \"../corpus/Turkish-English Parallel Corpus.txt\"\n",
    "DIM = 120       # dimension of word embeddings\n",
    "WS = 5          # size of the context window\n",
    "EPOCH = 5       # number of epochs\n",
    "NEG = 5         # number of negative examples\n",
    "LR = 0.05       # learning rate\n",
    "MINCOUNT = 5    # minimal number of word occurences\n",
    "MINN = 3        # min length of char ngram\n",
    "MAXN = 6        # max length of char ngram\n",
    "WNG = 2  # max length of word ngram\n",
    "LOSS = \"ns\"     # loss function {ns, hs, softmax} (negative sampling loss function, hierarchical softmax loss function, softmax loss function, one-vs-all loss function)\n",
    "OUTPUT = \"fasttext.model\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "```\n",
    "input             # training file path (required)\n",
    "model             # unsupervised fasttext model {cbow, skipgram} [skipgram]\n",
    "lr                # learning rate [0.05]\n",
    "dim               # size of word vectors [100]\n",
    "ws                # size of the context window [5]\n",
    "epoch             # number of epochs [5]\n",
    "minCount          # minimal number of word occurences [5]\n",
    "minn              # min length of char ngram [3]\n",
    "maxn              # max length of char ngram [6]\n",
    "neg               # number of negatives sampled [5]\n",
    "wordNgrams        # max length of word ngram [1]\n",
    "loss              # loss function {ns, hs, softmax, ova} [ns] \n",
    "bucket            # number of buckets [2000000]\n",
    "thread            # number of threads [number of cpus]\n",
    "lrUpdateRate      # change the rate of updates for the learning rate [100]\n",
    "t                 # sampling threshold [0.0001]\n",
    "verbose           # verbose [2]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "model = fasttext.train_unsupervised(input = INPUT, \n",
    "                                    model=MODEL,\n",
    "                                    dim = DIM,\n",
    "                                    ws = WS,\n",
    "                                    epoch = EPOCH,\n",
    "                                    neg = NEG,\n",
    "                                    lr = LR,\n",
    "                                    minCount = MINCOUNT,\n",
    "                                    minn = MINN,\n",
    "                                    maxn = MAXN,\n",
    "                                    wordNgrams = WNG,\n",
    "                                    loss = LOSS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "model.save_model(OUTPUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the fasttext model\n",
    "model = fasttext.load_model(OUTPUT)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "get_dimension           # Get the dimension (size) of a lookup vector (hidden layer).\n",
    "                        # This is equivalent to `dim` property.\n",
    "get_input_vector        # Given an index, get the corresponding vector of the Input Matrix.\n",
    "get_input_matrix        # Get a copy of the full input matrix of a Model.\n",
    "get_labels              # Get the entire list of labels of the dictionary\n",
    "                        # This is equivalent to `labels` property.\n",
    "get_line                # Split a line of text into words and labels.\n",
    "get_output_matrix       # Get a copy of the full output matrix of a Model.\n",
    "get_sentence_vector     # Given a string, get a single vector represenation. This function\n",
    "                        # assumes to be given a single line of text. We split words on\n",
    "                        # whitespace (space, newline, tab, vertical tab) and the control\n",
    "                        # characters carriage return, formfeed and the null character.\n",
    "get_subword_id          # Given a subword, return the index (within input matrix) it hashes to.\n",
    "get_subwords            # Given a word, get the subwords and their indicies.\n",
    "get_word_id             # Given a word, get the word id within the dictionary.\n",
    "get_word_vector         # Get the vector representation of word.\n",
    "get_words               # Get the entire list of words of the dictionary\n",
    "                        # This is equivalent to `words` property.\n",
    "is_quantized            # whether the model has been quantized\n",
    "predict                 # Given a string, get a list of labels and a list of corresponding probabilities.\n",
    "quantize                # Quantize the model reducing the size of the model and it's memory footprint.\n",
    "save_model              # Save the model to the given path\n",
    "test                    # Evaluate supervised model using file given by path\n",
    "test_label              # Return the precision and recall score for each label. \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.words   # bug: get rid of phrases like \\xa0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'kral' in model.get_words(): \n",
    "    print(model.get_word_vector('kral'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get nearest neighbors\n",
    "model.get_nearest_neighbors('adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word analogies\n",
    "# predict what is to Z as what X is to Y: model.get_analogies(X, Y, Z)\n",
    "model.get_analogies(\"İstanbul\", \"Türkiye\", \"Amerika\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_nearest_neighbors('İngiltere')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4e2634ad7c2ac4f622a87f31f725ee1a72d856d0d4a189ceba727e424656b242"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
