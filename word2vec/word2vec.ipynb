{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec Window Alignment\n",
    "* As of March 14th, 2023, the Gensim Word2Vec implementation does not support left or right-aligned windows. This means that we can only use centered windows where `n` words are taken from both the left and right neighbors of the current word. The Word2Vec class in Gensim takes the window size `n` as a parameter called window. To compare the performance of left and right-aligned implementations, we made some modifications to the Gensim architecture. Install the modified version using `pip`:\n",
    "   ```cmd\n",
    "   pip install git+https://github.com/KarahanS/custom-gensim.git@window-alignment\n",
    "   ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import logging\n",
    "from gensim.models import Word2Vec\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "from utils.utils import LineSentences\n",
    "from utils.utils import callback"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gensim Word2Vec\n",
    "_Documentation: https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec_\n",
    "* `sentences` _(iterable of iterables, optional)_: The sentences iterable can be simply a list of lists of tokens, but for larger corpora, consider an iterable that streams the sentences directly from disk/network. See `BrownCorpus`, `Text8Corpus` or `LineSentence` in word2vec module for such examples. See also the tutorial on data streaming in Python. If you don’t supply sentences, the model is left uninitialized – use if you plan to initialize it in some other way.\n",
    "* `vector_size` _(int, optional)_: Dimensionality of the word vectors.\n",
    "* `window` _(int, optional)_: Maximum distance between the current and predicted word within a sentence.\n",
    "* `min_count` _(int, optional)_: Ignores all words with total frequency lower than this.\n",
    "* `workers` _(int, optional)_: Use these many worker threads to train the model (=faster training with multicore machines).\n",
    "* `sg` _({0, 1}, optional)_: Training algorithm: 1 for skip-gram; otherwise CBOW.\n",
    "* `hs` _({0, 1}, optional)_: If 1, hierarchical softmax will be used for model training. If 0, and negative is non-zero, negative sampling will be used.\n",
    "* `negative` _(int, optional)_: If > 0, negative sampling will be used, the int for negative specifies how many “noise words” should be drawn (usually between 5-20). If set to 0, no negative sampling is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In short:\n",
    "\n",
    "| SG | HS | Negative | Training Algorithm |\n",
    "|----|----|----------|-------------------|\n",
    "| 1  | 1  |          | Skip-Gram Hierarchical Softmax |\n",
    "| 1  | 0  | $\\neq$ 0 | Skip-Gram Negative Sampling |\n",
    "| 1  | 0  | = 0 | No training |\n",
    "| 0  | 1  |          | CBOW Hierarchical Softmax |\n",
    "| 0  | 0  | $\\neq$ 0 | CBOW Negative Sampling |\n",
    "| 0 | 0  | = 0 | No training |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documentation: https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec\n",
    "# Assumption: Provided input is a txt file with one sentence per line.\n",
    "INPUT = [\"../corpus/turkish-texts-tokenized.txt\", \"../corpus/bounwebcorpus.txt\"]\n",
    "MIN_COUNT = 10  # ignore all words with total frequency lower than this\n",
    "EMB = 300       # dimensionality of word vectors\n",
    "WINDOW = 5      # maximum distance between the target and context word within a sentence\n",
    "EPOCH = 10      # number of iterations (epochs) over the corpus\n",
    "SG = 1          # training algorithm: 1 for skip-gram; otherwise CBOW\n",
    "HS = 0          # if 1, hierarchical softmax will be used for model training. If set to 0, and negative is non-zero, negative sampling will be used. If both of them 0, no training algorithm will be used.\n",
    "NEGATIVE = 5    # if > 0, negative sampling will be used, the int for negative specifies how many \"noise words\" should be drawn (usually between 5-20). If set to 0, no negative sampling is used.\n",
    "OUTPUT = \"word2vec_5epoch_leftaligned.model\"\n",
    "ALIGNMENT = -1  # -1 for left alignment, 1 for right alignment, 0 for centered alignment\n",
    "\n",
    "# So, if both hs and negative are set to 0, it means that no training algorithm will be used to learn the word embeddings. In this case, you will have to provide pre-trained word embeddings for the model to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-14 19:07:14,355 : INFO : collecting all words and their counts\n",
      "2023-03-14 19:07:14,357 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2023-03-14 19:07:14,358 : INFO : collected 1573 word types from a corpus of 2838 raw words and 9 sentences\n",
      "2023-03-14 19:07:14,359 : INFO : Creating a fresh vocabulary\n",
      "2023-03-14 19:07:14,360 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=10 retains 20 unique words (1.27% of original 1573, drops 1553)', 'datetime': '2023-03-14T19:07:14.360778', 'gensim': '4.3.1.dev0', 'python': '3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "2023-03-14 19:07:14,360 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=10 leaves 593 word corpus (20.89% of original 2838, drops 2245)', 'datetime': '2023-03-14T19:07:14.360778', 'gensim': '4.3.1.dev0', 'python': '3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "2023-03-14 19:07:14,361 : INFO : deleting the raw counts dictionary of 1573 items\n",
      "2023-03-14 19:07:14,361 : INFO : sample=0.001 downsamples 20 most-common words\n",
      "2023-03-14 19:07:14,362 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 89.54710015778741 word corpus (15.1%% of prior 593)', 'datetime': '2023-03-14T19:07:14.362779', 'gensim': '4.3.1.dev0', 'python': '3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "2023-03-14 19:07:14,362 : INFO : estimated required memory for 20 words and 300 dimensions: 58000 bytes\n",
      "2023-03-14 19:07:14,364 : INFO : resetting layer weights\n",
      "2023-03-14 19:07:14,364 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2023-03-14T19:07:14.364852', 'gensim': '4.3.1.dev0', 'python': '3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'build_vocab'}\n",
      "2023-03-14 19:07:14,365 : INFO : Word2Vec lifecycle event {'msg': 'training model with 8 workers on 20 vocabulary and 300 features, using sg=1 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2023-03-14T19:07:14.365847', 'gensim': '4.3.1.dev0', 'python': '3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'train'}\n",
      "2023-03-14 19:07:14,371 : INFO : EPOCH 0: training on 2838 raw words (82 effective words) took 0.0s, 55957 effective words/s\n",
      "2023-03-14 19:07:14,381 : INFO : EPOCH 1: training on 2838 raw words (104 effective words) took 0.0s, 33072 effective words/s\n",
      "2023-03-14 19:07:14,386 : INFO : EPOCH 2: training on 2838 raw words (103 effective words) took 0.0s, 52162 effective words/s\n",
      "2023-03-14 19:07:14,391 : INFO : EPOCH 3: training on 2838 raw words (88 effective words) took 0.0s, 42758 effective words/s\n",
      "2023-03-14 19:07:14,398 : INFO : EPOCH 4: training on 2838 raw words (77 effective words) took 0.0s, 36054 effective words/s\n",
      "2023-03-14 19:07:14,407 : INFO : EPOCH 5: training on 2838 raw words (104 effective words) took 0.0s, 35656 effective words/s\n",
      "2023-03-14 19:07:14,415 : INFO : EPOCH 6: training on 2838 raw words (75 effective words) took 0.0s, 26748 effective words/s\n",
      "2023-03-14 19:07:14,420 : INFO : EPOCH 7: training on 2838 raw words (78 effective words) took 0.0s, 48653 effective words/s\n",
      "2023-03-14 19:07:14,431 : INFO : EPOCH 8: training on 2838 raw words (88 effective words) took 0.0s, 23058 effective words/s\n",
      "2023-03-14 19:07:14,438 : INFO : EPOCH 9: training on 2838 raw words (79 effective words) took 0.0s, 45078 effective words/s\n",
      "2023-03-14 19:07:14,439 : INFO : Word2Vec lifecycle event {'msg': 'training on 28380 raw words (878 effective words) took 0.1s, 12115 effective words/s', 'datetime': '2023-03-14T19:07:14.439198', 'gensim': '4.3.1.dev0', 'python': '3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'train'}\n",
      "2023-03-14 19:07:14,440 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=20, vector_size=300, alpha=0.025>', 'datetime': '2023-03-14T19:07:14.440200', 'gensim': '4.3.1.dev0', 'python': '3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'}\n",
      "2023-03-14 19:07:14,441 : INFO : storing 20x300 projection weights into word2vec_10epoch.model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 0: 879.2654418945312\n",
      "Loss after epoch 1: 1132.3890991210938\n",
      "Loss after epoch 2: 960.003662109375\n",
      "Loss after epoch 3: 906.3017578125\n",
      "Loss after epoch 4: 728.04736328125\n",
      "Loss after epoch 5: 1011.609375\n",
      "Loss after epoch 6: 795.62060546875\n",
      "Loss after epoch 7: 799.21923828125\n",
      "Loss after epoch 8: 860.857421875\n",
      "Loss after epoch 9: 735.37060546875\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "model = Word2Vec(sentences=LineSentences(INPUT), \n",
    "                vector_size=EMB, \n",
    "                window=WINDOW, \n",
    "                min_count=MIN_COUNT, \n",
    "                epochs = EPOCH, \n",
    "                sg = SG,\n",
    "                hs = HS,\n",
    "                negative = NEGATIVE,\n",
    "                compute_loss=True,\n",
    "                window_alignment=ALIGNMENT,\n",
    "                workers=multiprocessing.cpu_count(), callbacks=[callback()])\n",
    "model.wv.save_word2vec_format(OUTPUT, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "word_vectors = KeyedVectors.load_word2vec_format(OUTPUT, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors.most_similar_cosmul(positive=['kadın', 'kral'], negative=['adam'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vocabulary file\n",
    "vocab = list(word_vectors.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actually, vocabulary is already sorted according to the frequency of the words. But, you can sort it again to be sure.\n",
    "word_counts = [word_vectors.get_vecattr(word, 'count') for word in vocab]  # get frequency of each word in corpus\n",
    "\n",
    "# Sort the vocabulary by word counts in descending order\n",
    "sorted_vocab = [word for _, word in sorted(zip(word_counts, vocab), reverse=True)]\n",
    "print(sorted_vocab[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write vocab to corpus/vocab.txt\n",
    "with open(\"../corpus/vocab.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for word in sorted_vocab:\n",
    "        f.write(word + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4e2634ad7c2ac4f622a87f31f725ee1a72d856d0d4a189ceba727e424656b242"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
