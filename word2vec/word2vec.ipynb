{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import multiprocessing\n",
    "import logging\n",
    "\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gensim Word2Vec\n",
    "_Documentation: https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec_\n",
    "* `sentences` _(iterable of iterables, optional)_: The sentences iterable can be simply a list of lists of tokens, but for larger corpora, consider an iterable that streams the sentences directly from disk/network. See `BrownCorpus`, `Text8Corpus` or `LineSentence` in word2vec module for such examples. See also the tutorial on data streaming in Python. If you don’t supply sentences, the model is left uninitialized – use if you plan to initialize it in some other way.\n",
    "* `vector_size` _(int, optional)_: Dimensionality of the word vectors.\n",
    "* `window` _(int, optional)_: Maximum distance between the current and predicted word within a sentence.\n",
    "* `min_count` _(int, optional)_: Ignores all words with total frequency lower than this.\n",
    "* `workers` _(int, optional)_: Use these many worker threads to train the model (=faster training with multicore machines).\n",
    "* `sg` _({0, 1}, optional)_: Training algorithm: 1 for skip-gram; otherwise CBOW.\n",
    "* `hs` _({0, 1}, optional)_: If 1, hierarchical softmax will be used for model training. If 0, and negative is non-zero, negative sampling will be used.\n",
    "* `negative` _(int, optional)_: If > 0, negative sampling will be used, the int for negative specifies how many “noise words” should be drawn (usually between 5-20). If set to 0, no negative sampling is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In short:\n",
    "\n",
    "| SG | HS | Negative | Training Algorithm |\n",
    "|----|----|----------|-------------------|\n",
    "| 1  | 1  |          | Skip-Gram Hierarchical Softmax |\n",
    "| 1  | 0  | $\\neq$ 0 | Skip-Gram Negative Sampling |\n",
    "| 1  | 0  | = 0 | No training |\n",
    "| 0  | 1  |          | CBOW Hierarchical Softmax |\n",
    "| 0  | 0  | $\\neq$ 0 | CBOW Negative Sampling |\n",
    "| 0 | 0  | = 0 | No training |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documentation: https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec\n",
    "# Assumption: Provided input is a txt file with one sentence per line.\n",
    "INPUT = [\"../corpus/turkish-texts-tokenized.txt\", \"../corpus/bounwebcorpus.txt\"]\n",
    "MIN_COUNT = 10   # ignore all words with total frequency lower than this\n",
    "EMB = 300       # dimensionality of word vectors\n",
    "WINDOW = 5      # maximum distance between the target and context word within a sentence\n",
    "EPOCH = 10       # number of iterations (epochs) over the corpus\n",
    "SG = 1          # training algorithm: 1 for skip-gram; otherwise CBOW\n",
    "HS = 0          # if 1, hierarchical softmax will be used for model training. If set to 0, and negative is non-zero, negative sampling will be used. If both of them 0, no training algorithm will be used.\n",
    "NEGATIVE = 5    # if > 0, negative sampling will be used, the int for negative specifies how many \"noise words\" should be drawn (usually between 5-20). If set to 0, no negative sampling is used.\n",
    "OUTPUT = \"word2vec_10epoch.model\"\n",
    "\n",
    "# So, if both hs and negative are set to 0, it means that no training algorithm will be used to learn the word embeddings. In this case, you will have to provide pre-trained word embeddings for the model to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LineSentences(object):\n",
    "    def __init__(self, filenames):\n",
    "        self.filenames = filenames\n",
    "    \n",
    "    # memory-friendly iterator\n",
    "    def __iter__(self):\n",
    "        for filename in self.filenames:\n",
    "            for line in open(filename, \"r\", encoding=\"utf-8\"):\n",
    "                yield line.strip().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "model = Word2Vec(sentences=LineSentences(INPUT), \n",
    "                vector_size=EMB, \n",
    "                window=WINDOW, \n",
    "                min_count=MIN_COUNT, \n",
    "                epochs = EPOCH, \n",
    "                sg = SG,\n",
    "                hs = HS,\n",
    "                negative = NEGATIVE,\n",
    "                compute_loss=True,\n",
    "                workers=multiprocessing.cpu_count())\n",
    "model.wv.save_word2vec_format(OUTPUT, binary=True)\n",
    "training_loss = model.get_latest_training_loss()\n",
    "print(training_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "word_vectors = KeyedVectors.load_word2vec_format(OUTPUT, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors.most_similar_cosmul(positive=['kadın', 'kral'], negative=['adam'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vocabulary file\n",
    "vocab = list(word_vectors.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',', '.', 've', 'bir', 'bu', 'da', 'de', 'için', 'iki', 'ile']\n"
     ]
    }
   ],
   "source": [
    "# Actually, vocabulary is already sorted according to the frequency of the words. But, you can sort it again to be sure.\n",
    "word_counts = [word_vectors.get_vecattr(word, 'count') for word in vocab]  # get frequency of each word in corpus\n",
    "\n",
    "# Sort the vocabulary by word counts in descending order\n",
    "sorted_vocab = [word for _, word in sorted(zip(word_counts, vocab), reverse=True)]\n",
    "print(sorted_vocab[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write vocab to corpus/vocab.txt\n",
    "with open(\"../corpus/vocab.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for word in sorted_vocab:\n",
    "        f.write(word + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4e2634ad7c2ac4f622a87f31f725ee1a72d856d0d4a189ceba727e424656b242"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
