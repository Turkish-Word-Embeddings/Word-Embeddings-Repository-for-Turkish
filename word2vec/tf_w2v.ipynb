{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BuQIIZZ9RCAX"
      },
      "outputs": [],
      "source": [
        "# Word2Vec Negative Sample implementation\n",
        "# paper: https://arxiv.org/abs/1301.3781\n",
        "# source: https://www.tensorflow.org/tutorials/text/word2vec\n",
        "\n",
        "import tensorflow as tf\n",
        "import io\n",
        "import re \n",
        "import string\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Constants and corpus file\n",
        "BATCH = 1024                # batch size\n",
        "EMB = 128                   # embedding dimensions (number of features for each word)\n",
        "EPOCH = 20                  # number of epochs for training\n",
        "SEQUENCE_LENGTH = 10        # maximum length of a sentence\n",
        "VOCAB_SIZE = 4096           # number of unique words in the corpus\n",
        "NUM_NS = 4                  # number of negative samples per positive context.\n",
        "WIN_SIZE = 2                # window size for skip-gram\n",
        "BUFFER = 10000              # buffer size for shuffling\n",
        "FILE = \"../corpus/Turkish-English Parallel Corpus.txt\"  # corpus file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WonbZZnn_jpW"
      },
      "outputs": [],
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqip3VoedPik"
      },
      "outputs": [],
      "source": [
        "# Generates skip-gram pairs with negative sampling for a list of sequences\n",
        "# (int-encoded sentences) based on window size, number of negative samples and vocabulary size.\n",
        "def generate_training_data(sequences, window_size, num_negative_samples, vocab_size):\n",
        "  targets, contexts, labels = [], [], []\n",
        "  sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)  #  Zipf's distribution\n",
        "\n",
        "  # Iterate over all sequences (sentences) in the dataset.\n",
        "  for sequence in sequences:\n",
        "    # Generate positive skip-gram pairs for a sequence (sentence).\n",
        "    positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
        "          sequence,\n",
        "          vocabulary_size=vocab_size,\n",
        "          sampling_table=sampling_table,\n",
        "          window_size=window_size,\n",
        "          negative_samples=0)\n",
        "\n",
        "    # Iterate over each positive skip-gram pair to produce training examples\n",
        "    # with a positive context word and negative samples.\n",
        "    for target_word, context_word in positive_skip_grams:\n",
        "      context_class = tf.expand_dims(\n",
        "          tf.constant([context_word], dtype=\"int64\"), 1)\n",
        "      negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
        "          true_classes=context_class,\n",
        "          num_true=1,\n",
        "          num_sampled=num_negative_samples,\n",
        "          unique=True,\n",
        "          range_max=vocab_size,\n",
        "          name=\"negative_sampling\")\n",
        "\n",
        "      # Build context and label vectors (for one target word)\n",
        "      context = tf.concat([tf.squeeze(context_class,1), negative_sampling_candidates], 0)\n",
        "      label = tf.constant([1] + [0]*num_negative_samples, dtype=\"int64\")\n",
        "\n",
        "      # Append each element from the training example to global lists.\n",
        "      targets.append(target_word)\n",
        "      contexts.append(context)\n",
        "      labels.append(label)\n",
        "\n",
        "  return targets, contexts, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U04oAET546Sb"
      },
      "outputs": [],
      "source": [
        "def custom_standardization(input_data):\n",
        "  lowercase = tf.strings.lower(input_data)\n",
        "  return tf.strings.regex_replace(lowercase,'[%s]' % re.escape(string.punctuation), '')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJuE5Mky_x7d",
        "outputId": "382f12fd-f224-40ce-d60d-d729528184bd"
      },
      "outputs": [],
      "source": [
        "with open(FILE, encoding = 'utf-8') as f:\n",
        "  lines = f.read().splitlines()\n",
        "for line in lines[5000:5005]:\n",
        "  print(line)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CHuQ9TvQ5G__"
      },
      "outputs": [],
      "source": [
        "dataset = tf.data.TextLineDataset(FILE).filter(lambda x: tf.cast(tf.strings.length(x), bool))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NoNXzsau5KxE"
      },
      "outputs": [],
      "source": [
        "# find maximum length of a sentence in the dataset\n",
        "vectorize_layer = tf.keras.layers.TextVectorization(\n",
        "                standardize=custom_standardization,\n",
        "                max_tokens=VOCAB_SIZE,\n",
        "                output_mode='int',\n",
        "                output_sequence_length=SEQUENCE_LENGTH)\n",
        "                \n",
        "vectorize_layer.adapt(dataset.batch(BATCH))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-4RfiMe8dHe",
        "outputId": "92df53e4-2ff8-46e9-aeb7-d4c1737b6415"
      },
      "outputs": [],
      "source": [
        "inverse_vocab = vectorize_layer.get_vocabulary()\n",
        "inverse_vocab[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l8iluxa38hoJ"
      },
      "outputs": [],
      "source": [
        "# Vectorize the data in text_ds.\n",
        "text_vector_ds = dataset.batch(BATCH).prefetch(tf.data.AUTOTUNE).map(vectorize_layer).unbatch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bH4prNcQ5KzD",
        "outputId": "922a42b3-d570-443e-ffc8-d623dbeb5705"
      },
      "outputs": [],
      "source": [
        "sequences = list(text_vector_ds.as_numpy_iterator())\n",
        "len(sequences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GvkcGkio5K34"
      },
      "outputs": [],
      "source": [
        "targets, contexts, labels = generate_training_data(\n",
        "    sequences=sequences, \n",
        "    window_size=WIN_SIZE, \n",
        "    num_negative_samples=NUM_NS, \n",
        "    vocab_size=VOCAB_SIZE)\n",
        "\n",
        "targets = np.array(targets)\n",
        "contexts = np.array(contexts)\n",
        "labels = np.array(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PCxWiJks5Qb0"
      },
      "outputs": [],
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
        "dataset = dataset.shuffle(BUFFER).batch(BATCH, drop_remainder=True)\n",
        "dataset = dataset.cache().prefetch(buffer_size=tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0C95E5__XVx"
      },
      "outputs": [],
      "source": [
        "class Word2Vec(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, num_ns=4):\n",
        "    super(Word2Vec, self).__init__()\n",
        "    self.target_embedding = tf.keras.layers.Embedding(vocab_size,\n",
        "                                      embedding_dim,\n",
        "                                      input_length=1,\n",
        "                                      name=\"w2v_embedding\")\n",
        "    self.context_embedding = tf.keras.layers.Embedding(vocab_size,\n",
        "                                      embedding_dim,\n",
        "                                      input_length=num_ns+1)\n",
        "\n",
        "  def call(self, pair):\n",
        "    target, context = pair\n",
        "\n",
        "    # context: (batch, context)\n",
        "    if len(target.shape) == 2:\n",
        "      target = tf.squeeze(target, axis=1)\n",
        "    # target: (batch,)\n",
        "    word_emb = self.target_embedding(target)               # word_emb: (batch size, embedding size)\n",
        "    context_emb = self.context_embedding(context)          # context_emb: (batch size, context size, embedding size)\n",
        "    dots = tf.einsum('be,bce->bc', word_emb, context_emb)  # dots: (batch size, context size)\n",
        "    return dots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjcxaNht5QfB",
        "outputId": "3bf104e5-be6b-4e17-df09-16c255ee14fc"
      },
      "outputs": [],
      "source": [
        "word2vec = Word2Vec(vocab_size = VOCAB_SIZE, embedding_dim = EMB, num_ns=NUM_NS)\n",
        "word2vec.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")\n",
        "word2vec.fit(dataset, epochs=EPOCH, callbacks=[callback], verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8LkVqb7U_fX6"
      },
      "outputs": [],
      "source": [
        "#docs_infra: no_execute\n",
        "%tensorboard --logdir logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1doXQqwk5Qi0"
      },
      "outputs": [],
      "source": [
        "weights = word2vec.get_layer('w2v_embedding').get_weights()[0]\n",
        "vocab = vectorize_layer.get_vocabulary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMlh8dR568ga"
      },
      "outputs": [],
      "source": [
        "out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\n",
        "out_m = io.open('metadata.tsv', 'w', encoding='utf-8')\n",
        "\n",
        "for index, word in enumerate(vocab):\n",
        "    if (index == 0): continue  # skip 0, it's padding.\n",
        "    vec = weights[index]\n",
        "    out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
        "    out_m.write(word + \"\\n\")\n",
        "out_v.close()\n",
        "out_m.close()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    },
    "vscode": {
      "interpreter": {
        "hash": "4e2634ad7c2ac4f622a87f31f725ee1a72d856d0d4a189ceba727e424656b242"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
