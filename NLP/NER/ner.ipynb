{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_only_consistent(path):\n",
    "    # create a dataframe consisting of word and PoS tag\n",
    "    sentences = []\n",
    "    with open(path, 'r', encoding=\"utf-8\") as f:\n",
    "        data = f.readlines()\n",
    "        r = len(data)\n",
    "        i = 0\n",
    "        while(i < r):\n",
    "            sentence = []\n",
    "            ne_representation = []\n",
    "            while(i < r and data[i] != \"\\n\"):\n",
    "                parts = data[i].split(\" \")\n",
    "                word = parts[0].strip()\n",
    "                ne = parts[-1].strip()\n",
    "\n",
    "                ne_representation.append(ne)\n",
    "                sentence.append(word) \n",
    "                i += 1\n",
    "            i+=1\n",
    "            sentences.append((sentence, ne_representation))\n",
    "        return sentences\n",
    "\n",
    "# read test, dev and train csv files\n",
    "train = read_only_consistent(\"data/gungor.ner.train.14.only_consistent\")\n",
    "dev = read_only_consistent(\"data/gungor.ner.dev.14.only_consistent\")\n",
    "test = read_only_consistent(\"data/gungor.ner.test.14.only_consistent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "sentence, pos_repr = random.choice(train)\n",
    "print(sentence, \":\", pos_repr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create a dataframe\n",
    "traindf = pd.DataFrame(\n",
    "    {'sentence': [i[0] for i in train],\n",
    "        'ne': [i[1] for i in train]\n",
    "    })\n",
    "\n",
    "testdf = pd.DataFrame(\n",
    "    {'sentence': [i[0] for i in test],\n",
    "        'ne': [i[1] for i in test]\n",
    "    })\n",
    "\n",
    "devdf = pd.DataFrame(\n",
    "    {'sentence': [i[0] for i in dev],\n",
    "        'ne': [i[1] for i in dev]\n",
    "    })\n",
    "\n",
    "\n",
    "traindf.head(1), devdf.head(1), testdf.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainx = traindf['sentence']\n",
    "trainy = traindf['ne']\n",
    "\n",
    "trainx[0], trainy[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### VECTORIZATION #######\n",
    "word_tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "word_tokenizer.fit_on_texts(trainx)                    # fit tokeniser on data\n",
    "X_encoded = word_tokenizer.texts_to_sequences(trainx)  # use the tokeniser to encode input sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### VECTORIZATION #######\n",
    "tag_tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tag_tokenizer.fit_on_texts(trainy)                    # fit tokeniser on data\n",
    "Y_encoded = tag_tokenizer.texts_to_sequences(trainy)  # use the tokeniser to encode input sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure that each sequence of input and output is same length\n",
    "\n",
    "different_length = [1 if len(input) != len(output) else 0 for input, output in zip(X_encoded, Y_encoded)]\n",
    "print(\"{} sentences have disparate input-output lengths.\".format(sum(different_length)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = [len(seq) for seq in X_encoded]\n",
    "print(\"Length of longest sentence: {}\".format(max(lengths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.boxplot(lengths, orient='h')\n",
    "plt.title(\"Boxplot of sentence lengths\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH = 130  # sequences greater than MAX_SEQ_LENGTH in length will be truncated\n",
    "\n",
    "X_padded = tf.keras.preprocessing.sequence.pad_sequences(X_encoded, maxlen=MAX_SEQ_LENGTH, padding=\"pre\", truncating=\"post\")\n",
    "Y_padded = tf.keras.preprocessing.sequence.pad_sequences(Y_encoded, maxlen=MAX_SEQ_LENGTH, padding=\"pre\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_padded[0], \"\\n\"*3)\n",
    "print(Y_padded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"C:/Users/karab/Desktop/Models/glove.txt\"\n",
    "EMBEDDING_SIZE  = 300  # each word in word2vec model is represented using a 300 dimensional vector\n",
    "VOCABULARY_SIZE = len(word_tokenizer.word_index) + 1\n",
    "\n",
    "#load fasttext embeddings\n",
    "print('loading word embeddings...')\n",
    "word_vectors = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    MODEL,\n",
    "    binary=False,\n",
    "    no_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an empty embedding matrix\n",
    "embedding_weights = np.zeros((VOCABULARY_SIZE, EMBEDDING_SIZE))\n",
    "word2id = word_tokenizer.word_index\n",
    "for word, index in word2id.items():\n",
    "    try:  embedding_weights[index, :] = word_vectors[word]\n",
    "    except KeyError: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Embeddings shape: {}\".format(embedding_weights.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_final = tf.keras.utils.to_categorical(Y_padded)\n",
    "# print Y of the first output sequence\n",
    "print(Y_final.shape)  # (number of sequences, length of each sequence, number of entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Perform all the operations for dev set ###\n",
    "devx = devdf['sentence']\n",
    "devy = devdf['ne']\n",
    "\n",
    "devx_encoded = word_tokenizer.texts_to_sequences(devx)\n",
    "devy_encoded = tag_tokenizer.texts_to_sequences(devy)\n",
    "\n",
    "devx_padded = tf.keras.preprocessing.sequence.pad_sequences(devx_encoded, maxlen=MAX_SEQ_LENGTH, padding=\"pre\", truncating=\"post\")\n",
    "devy_padded = tf.keras.preprocessing.sequence.pad_sequences(devy_encoded, maxlen=MAX_SEQ_LENGTH, padding=\"pre\", truncating=\"post\")\n",
    "\n",
    "devy_final= tf.keras.utils.to_categorical(devy_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Perform all the operations for test set ###\n",
    "testx = testdf['sentence']\n",
    "testy = testdf['ne']\n",
    "\n",
    "testx_encoded = word_tokenizer.texts_to_sequences(testx)\n",
    "testy_encoded = tag_tokenizer.texts_to_sequences(testy)\n",
    "\n",
    "testx_padded = tf.keras.preprocessing.sequence.pad_sequences(testx_encoded, maxlen=MAX_SEQ_LENGTH, padding=\"pre\", truncating=\"post\")\n",
    "testy_padded = tf.keras.preprocessing.sequence.pad_sequences(testy_encoded, maxlen=MAX_SEQ_LENGTH, padding=\"pre\", truncating=\"post\")\n",
    "\n",
    "testy_final = tf.keras.utils.to_categorical(testy_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TRAINING DATA\")\n",
    "print('Shape of input sequences: {}'.format(X_padded.shape))\n",
    "print('Shape of output sequences: {}'.format(Y_final.shape))\n",
    "print(\"-\"*50)\n",
    "print(\"VALIDATION DATA\")\n",
    "print('Shape of input sequences: {}'.format(devx_padded.shape))\n",
    "print('Shape of output sequences: {}'.format(devy_final.shape))\n",
    "print(\"-\"*50)\n",
    "print(\"TESTING DATA\")\n",
    "print('Shape of input sequences: {}'.format(testx_padded.shape))\n",
    "print('Shape of output sequences: {}'.format(testy_final.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print unique elements in trainy\n",
    "print(\"Unique elements in trainy: {}\".format(set([item for sublist in trainy for item in sublist])))\n",
    "# print unique elements in devy\n",
    "print(\"Unique elements in devy: {}\".format(set([item for sublist in devy for item in sublist])))\n",
    "# print unique elements in testy\n",
    "print(\"Unique elements in testy: {}\".format(set([item for sublist in testy for item in sublist])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total number of tags\n",
    "NUM_CLASSES = Y_final.shape[2]\n",
    "NUM_CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Bidirectional, TimeDistributed, Embedding\n",
    "\n",
    "bidirect_model = Sequential()\n",
    "bidirect_model.add(Embedding(input_dim     = VOCABULARY_SIZE,\n",
    "                             output_dim    = EMBEDDING_SIZE,\n",
    "                             input_length  = MAX_SEQ_LENGTH,\n",
    "                             weights       = [embedding_weights],\n",
    "                             trainable     = False\n",
    "))\n",
    "bidirect_model.add(Bidirectional(LSTM(16, return_sequences=True)))\n",
    "bidirect_model.add(TimeDistributed(Dense(NUM_CLASSES, activation='softmax')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bidirect_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bidirect_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bidirect_training = bidirect_model.fit(X_padded, Y_final, batch_size=128, epochs=10, validation_data=(devx_padded, devy_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise training history\n",
    "plt.plot(bidirect_training.history['acc'])\n",
    "plt.plot(bidirect_training.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.grid()\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = bidirect_model.evaluate(testx_padded, testy_final, verbose = 1)\n",
    "print(\"Loss: {0},\\nAccuracy: {1}\".format(loss, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
