{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\karab\\Desktop\\turkish-word-embeddings\\env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tensorflow as tf  # pytorch for the model, tensorflow for tokenizer\n",
    "import gensim\n",
    "from dataset import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1+cu117\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 3050 Ti Laptop GPU'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read train and test csv\n",
    "traindf = pd.read_csv(\"train.csv\")\n",
    "testdf = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# get label\n",
    "trainy = traindf['sentiment'].values\n",
    "testy = testdf['sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows:  16155\n",
      "Number of rows with kfold = 0 3231\n",
      "Number of rows with kfold = 1 3231\n",
      "Number of rows with kfold = 2 3231\n",
      "Number of rows with kfold = 3 3231\n",
      "Number of rows with kfold = 4 3231\n"
     ]
    }
   ],
   "source": [
    "# K-FOLD CROSS VALIDATION SETUP\n",
    "kf = model_selection.StratifiedKFold(n_splits=5)\n",
    "\n",
    "# assign folds to [0, 1, 2, 3, 4]\n",
    "for fold, (train_, valid_) in enumerate(kf.split(X=traindf, y=trainy)):\n",
    "    traindf.loc[valid_, 'kfold'] = fold\n",
    "\n",
    "print(\"Total number of rows: \", traindf.shape[0])\n",
    "print(\"Number of rows with kfold = 0\", traindf[traindf.kfold==0].shape[0])\n",
    "print(\"Number of rows with kfold = 1\", traindf[traindf.kfold==1].shape[0])\n",
    "print(\"Number of rows with kfold = 2\", traindf[traindf.kfold==2].shape[0])\n",
    "print(\"Number of rows with kfold = 3\", traindf[traindf.kfold==3].shape[0])\n",
    "print(\"Number of rows with kfold = 4\", traindf[traindf.kfold==4].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"C:/Users/karab/Desktop/Models/fasttext-10ep.wordvectors\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word embeddings...\n"
     ]
    }
   ],
   "source": [
    "#load fasttext embeddings\n",
    "print('loading word embeddings...')\n",
    "word_vectors = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "            MODEL,\n",
    "            binary=True,\n",
    "            no_header=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1573013\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(word_vectors))\n",
    "word_vectors[\"bilgisayar\"].shape  # 300 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, embedding_matrix):\n",
    "        super(LSTM, self).__init__()\n",
    "        # Number of words = number of rows in embedding matrix\n",
    "        num_words = embedding_matrix.shape[0]\n",
    "        # Dimension of embedding is num of columns in the matrix\n",
    "        embedding_dim = embedding_matrix.shape[1]\n",
    "        # Define an input embedding layer\n",
    "        self.embedding = nn.Embedding(num_embeddings=num_words,\n",
    "                                      embedding_dim=embedding_dim)\n",
    "        # Embedding matrix actually is collection of parameter\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype = torch.float32))\n",
    "        # Because we use pretrained embedding (GLove, Fastext,etc) so we turn off requires_grad-meaning we do not train gradient on embedding weight\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        # LSTM with hidden_size = 128\n",
    "        self.lstm = nn.LSTM(\n",
    "                            embedding_dim, \n",
    "                            128,\n",
    "                            bidirectional=True,\n",
    "                            batch_first=True,\n",
    "                             )\n",
    "        # Input(512) because we use bi-directional LSTM ==> hidden_size*2 + maxpooling **2  = 128*4 = 512, will be explained more on forward method\n",
    "        self.out = nn.Linear(512, 1)\n",
    "    def forward(self, x):\n",
    "        # pass input (tokens) through embedding layer\n",
    "        x = self.embedding(x)\n",
    "        # fit embedding to LSTM\n",
    "        hidden, _ = self.lstm(x)\n",
    "        # apply mean and max pooling on lstm output\n",
    "        avg_pool= torch.mean(hidden, 1)\n",
    "        max_pool, index_max_pool = torch.max(hidden, 1)\n",
    "        # concat avg_pool and max_pool (so we have 256 size, also because this is bidirectional ==> 256*2 = 512)\n",
    "        out = torch.cat((avg_pool, max_pool), 1)\n",
    "        # fit out to self.out to conduct dimensionality reduction from 512 to 1\n",
    "        out = self.out(out)\n",
    "        # return output\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_loader, model, optimizer, device):\n",
    "    \"\"\"\n",
    "    this is model training for one epoch\n",
    "    data_loader:  this is torch dataloader, just like dataset but in torch and devide into batches\n",
    "    model : lstm\n",
    "    optimizer : torch optimizer : adam\n",
    "    device:  cuda or cpu\n",
    "    \"\"\"\n",
    "    # set model to training mode\n",
    "    model.train()\n",
    "    # go through batches of data in data loader\n",
    "    for data in data_loader:\n",
    "        reviews = data['review']\n",
    "        targets = data['target']\n",
    "        # move the data to device that we want to use\n",
    "        reviews = reviews.to(device, dtype = torch.long)\n",
    "        targets = targets.to(device, dtype = torch.float)\n",
    "        # clear the gradient\n",
    "        optimizer.zero_grad()\n",
    "        # make prediction from model\n",
    "        predictions = model(reviews)\n",
    "        # caculate the losses\n",
    "        loss = nn.BCEWithLogitsLoss()(predictions, targets.view(-1,1))\n",
    "        # backprob\n",
    "        loss.backward()\n",
    "        #single optimization step\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_loader, model, device):\n",
    "    final_predictions = []\n",
    "    final_targets = []\n",
    "    model.eval()\n",
    "    # turn off gradient calculation\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            reviews = data['review']\n",
    "            targets = data['target']\n",
    "            reviews = reviews.to(device, dtype = torch.long)\n",
    "            targets = targets.to(device, dtype=torch.float)\n",
    "            # make prediction\n",
    "            predictions = model(reviews)\n",
    "            # move prediction and target to cpu\n",
    "            predictions = predictions.cpu().numpy().tolist()\n",
    "            targets = data['target'].cpu().numpy().tolist()\n",
    "            # add predictions to final_prediction\n",
    "            final_predictions.extend(predictions)\n",
    "            final_targets.extend(targets)\n",
    "    return final_predictions, final_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 128 # maximum length for a sentence\n",
    "DIM = 300\n",
    "TRAIN_BATCH_SIZE = 16\n",
    "VALID_BATCH_SIZE = 8\n",
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(word_index, embedding_dict=None, dim=300):\n",
    "    \"\"\"\n",
    "     this function create the embedding matrix save in numpy array\n",
    "    :param word_index: a dictionary with word: index_value\n",
    "    :param embedding_dict: a dict with word embedding\n",
    "    :d_model: the dimension of word pretrained embedding\n",
    "    :return a numpy array with embedding vectors for all known words\n",
    "    \"\"\"\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, dim))\n",
    "    ## loop over all the words\n",
    "    for word, index in word_index.items():\n",
    "        if word in embedding_dict:\n",
    "            embedding_matrix[index] = embedding_dict[word]\n",
    "    return embedding_matrix\n",
    "\n",
    "# embedding_dict['word'] = vector\n",
    "# word_index['word'] = index\n",
    "# embedding_matrix[index] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['uzun süredir beyazperde ye konmasını bekledğim. kesinlikle izlenmesi gereken bir film. hatırladğım kadarıyla oyuncuların çoğu halkın içinden seçilmiştir. çok etkileyici...\\n',\n",
       " 'tek kelime: B A Ş Y A P I T .hanginiz dünyadaki en iyi filmleri konusurken bu filmi es geciyonuz ki....\\n',\n",
       " 'çok keyifli bi filmdi izlerken hiç sıkılmadım.10/10 filmden alıntı’eğer bir toplantı sensiz başlamıyorsa gitmeye değerdir’\\n',\n",
       " 'sürükleyici ve kaçırılmaması gereken bir film. 9/10\\n',\n",
       " 'filmden o kadar etkilenmiştim ki...özellikle özgürlük uğruna savaşan william wallace’in işkence edilerek idam edilmesi...ve sonunda özgürlük diye haykırışı...halan unutamıyorum...\\n']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindf['review'].values.tolist()[:5] # list of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Tokenization\n",
    "# use tf.keras for tokenization,  \n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(traindf['review'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load embeddings\n",
      "training model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:09<00:36,  9.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD:0, epoch: 0, accuracy_score: 0.8811513463324049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:18<00:28,  9.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD:0, epoch: 1, accuracy_score: 0.8848653667595172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [00:27<00:17,  8.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD:0, epoch: 2, accuracy_score: 0.8966264314453729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [00:34<00:08,  8.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD:0, epoch: 3, accuracy_score: 0.8978644382544104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:44<00:00,  8.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD:0, epoch: 4, accuracy_score: 0.8947694212318168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:09<00:38,  9.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD:1, epoch: 0, accuracy_score: 0.887341380377592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:18<00:27,  9.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD:1, epoch: 1, accuracy_score: 0.8938409161250387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [00:27<00:17,  8.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD:1, epoch: 2, accuracy_score: 0.8938409161250387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [00:35<00:08,  8.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD:1, epoch: 3, accuracy_score: 0.9052924791086351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:42<00:00,  8.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD:1, epoch: 4, accuracy_score: 0.9021974620860415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:07<00:28,  7.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD:2, epoch: 0, accuracy_score: 0.8644382544103992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:14<00:21,  7.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD:2, epoch: 1, accuracy_score: 0.8987929433611885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [00:21<00:14,  7.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD:2, epoch: 2, accuracy_score: 0.8972454348498917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [00:29<00:07,  7.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD:2, epoch: 3, accuracy_score: 0.89322191272052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:36<00:00,  7.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD:2, epoch: 4, accuracy_score: 0.9012689569792633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:07<00:29,  7.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD:3, epoch: 0, accuracy_score: 0.889817393995667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:14<00:21,  7.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD:3, epoch: 1, accuracy_score: 0.8953884246363355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [00:21<00:14,  7.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD:3, epoch: 2, accuracy_score: 0.8947694212318168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [00:28<00:07,  7.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD:3, epoch: 3, accuracy_score: 0.9003404518724853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:36<00:00,  7.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD:3, epoch: 4, accuracy_score: 0.9065304859176726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:07<00:28,  7.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD:4, epoch: 0, accuracy_score: 0.8656762612194367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:14<00:21,  7.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD:4, epoch: 1, accuracy_score: 0.8724852986691427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [00:21<00:14,  7.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD:4, epoch: 2, accuracy_score: 0.8656762612194367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [00:28<00:07,  7.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD:4, epoch: 3, accuracy_score: 0.861652739090065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:36<00:00,  7.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD:4, epoch: 4, accuracy_score: 0.8839368616527391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('Load embeddings')\n",
    "embedding_matrix = create_embedding_matrix(tokenizer.word_index, embedding_dict=word_vectors, dim=DIM)\n",
    "\n",
    "\n",
    "for fold in range(5):\n",
    "    # STEP 2: cross validation\n",
    "    train_df = traindf[traindf.kfold != fold].reset_index(drop=True)\n",
    "    valid_df = traindf[traindf.kfold == fold].reset_index(drop=True)\n",
    "    \n",
    "    # STEP 3: pad sequence\n",
    "    xtrain = tokenizer.texts_to_sequences(train_df.review.values)\n",
    "    xtest = tokenizer.texts_to_sequences(valid_df.review.values)\n",
    "    \n",
    "    # zero padding\n",
    "    xtrain = tf.keras.preprocessing.sequence.pad_sequences(xtrain, maxlen=MAX_LEN)\n",
    "    xtest = tf.keras.preprocessing.sequence.pad_sequences(xtest, maxlen=MAX_LEN)\n",
    "    \n",
    "    # STEP 4: initialize dataset class for training\n",
    "    train_dataset = Dataset(reviews=xtrain, targets=train_df['sentiment'].values)\n",
    "    \n",
    "    # STEP 5: Load dataset to Pytorch DataLoader\n",
    "    # after we have train_dataset, we create a torch dataloader to load train_dataset class based on specified batch_size\n",
    "    train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size = TRAIN_BATCH_SIZE, num_workers=2)\n",
    "    # initialize dataset class for validation\n",
    "    valid_dataset = Dataset(reviews=xtest, targets=valid_df['sentiment'].values)\n",
    "    valid_data_loader = torch.utils.data.DataLoader(valid_dataset, batch_size = VALID_BATCH_SIZE, num_workers=1)\n",
    "    \n",
    "    # STEP 6: Running \n",
    "    device = torch.device('cuda')\n",
    "    # feed embedding matrix to lstm\n",
    "    model_fasttext = LSTM(embedding_matrix)\n",
    "    # set model to cuda device\n",
    "    model_fasttext.to(device)\n",
    "    # initialize Adam optimizer\n",
    "    optimizer = torch.optim.Adam(model_fasttext.parameters(), lr=1e-3)\n",
    "    \n",
    "    print('training model')\n",
    "\n",
    "    for epoch in tqdm(range(EPOCHS)):\n",
    "        # train one epoch\n",
    "        train(train_data_loader, model_fasttext, optimizer, device)\n",
    "        # validate\n",
    "        outputs, targets = evaluate(valid_data_loader, model_fasttext, device)\n",
    "        # threshold\n",
    "        outputs = np.array(outputs) >= 0.5\n",
    "        # calculate accuracy\n",
    "        accuracy = metrics.accuracy_score(targets, outputs)\n",
    "        print(f'FOLD:{fold}, epoch: {epoch}, accuracy_score: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score: 0.8987527512839325\n"
     ]
    }
   ],
   "source": [
    "# test on test set testdf and testy\n",
    "xtest = tokenizer.texts_to_sequences(testdf.review.values)\n",
    "xtest = tf.keras.preprocessing.sequence.pad_sequences(xtest, maxlen=MAX_LEN)\n",
    "test_dataset = Dataset(reviews=xtest, targets=testy)\n",
    "test_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size = VALID_BATCH_SIZE, num_workers=1)\n",
    "outputs, targets = evaluate(test_data_loader, model_fasttext, device)\n",
    "outputs = np.array(outputs) >= 0.5\n",
    "accuracy = metrics.accuracy_score(targets, outputs)\n",
    "print(f'accuracy_score: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
