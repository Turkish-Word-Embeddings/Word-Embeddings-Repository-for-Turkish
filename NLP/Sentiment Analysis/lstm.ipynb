{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\karab\\Desktop\\turkish-word-embeddings\\env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import io\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer \n",
    "import os, re, csv, math\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tensorflow as tf  # pytorch for the model, tensorflow for tokenizer\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "from dataset import IMDBDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1+cu117\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 3050 Ti Laptop GPU'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read train and test csv\n",
    "traindf = pd.read_csv(\"train.csv\")\n",
    "testdf = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# get label\n",
    "trainy = traindf['sentiment'].values\n",
    "testy = testdf['sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows:  16155\n",
      "Number of rows with kfold = 0 3231\n",
      "Number of rows with kfold = 1 3231\n",
      "Number of rows with kfold = 2 3231\n",
      "Number of rows with kfold = 3 3231\n",
      "Number of rows with kfold = 4 3231\n"
     ]
    }
   ],
   "source": [
    "# K-FOLD CROSS VALIDATION SETUP\n",
    "kf = model_selection.StratifiedKFold(n_splits=5)\n",
    "\n",
    "# assign folds to [0, 1, 2, 3, 4]\n",
    "for fold, (train_, valid_) in enumerate(kf.split(X=traindf, y=trainy)):\n",
    "    traindf.loc[valid_, 'kfold'] = fold\n",
    "\n",
    "print(\"Total number of rows: \", traindf.shape[0])\n",
    "print(\"Number of rows with kfold = 0\", traindf[traindf.kfold==0].shape[0])\n",
    "print(\"Number of rows with kfold = 1\", traindf[traindf.kfold==1].shape[0])\n",
    "print(\"Number of rows with kfold = 2\", traindf[traindf.kfold==2].shape[0])\n",
    "print(\"Number of rows with kfold = 3\", traindf[traindf.kfold==3].shape[0])\n",
    "print(\"Number of rows with kfold = 4\", traindf[traindf.kfold==4].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"../../utils/word2vec-fasttext-average-epoch10.wordvectors\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word embeddings...\n"
     ]
    }
   ],
   "source": [
    "#load fasttext embeddings\n",
    "print('loading word embeddings...')\n",
    "word_vectors = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "            MODEL,\n",
    "            binary=True,\n",
    "            no_header=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1573013\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(word_vectors))\n",
    "word_vectors[\"bilgisayar\"].shape  # 300 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, embedding_matrix):\n",
    "        super(LSTM, self).__init__()\n",
    "        # Number of words = number of rows in embedding matrix\n",
    "        num_words = embedding_matrix.shape[0]\n",
    "        # Dimension of embedding is num of columns in the matrix\n",
    "        embedding_dim = embedding_matrix.shape[1]\n",
    "        # Define an input embedding layer\n",
    "        self.embedding = nn.Embedding(num_embeddings=num_words,\n",
    "                                      embedding_dim=embedding_dim)\n",
    "        # Embedding matrix actually is collection of parameter\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype = torch.float32))\n",
    "        # Because we use pretrained embedding (GLove, Fastext,etc) so we turn off requires_grad-meaning we do not train gradient on embedding weight\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        # LSTM with hidden_size = 128\n",
    "        self.lstm = nn.LSTM(\n",
    "                            embedding_dim, \n",
    "                            128,\n",
    "                            bidirectional=True,\n",
    "                            batch_first=True,\n",
    "                             )\n",
    "        # Input(512) because we use bi-directional LSTM ==> hidden_size*2 + maxpooling **2  = 128*4 = 512, will be explained more on forward method\n",
    "        self.out = nn.Linear(512, 1)\n",
    "    def forward(self, x):\n",
    "        # pass input (tokens) through embedding layer\n",
    "        x = self.embedding(x)\n",
    "        # fit embedding to LSTM\n",
    "        hidden, _ = self.lstm(x)\n",
    "        # apply mean and max pooling on lstm output\n",
    "        avg_pool= torch.mean(hidden, 1)\n",
    "        max_pool, index_max_pool = torch.max(hidden, 1)\n",
    "        # concat avg_pool and max_pool (so we have 256 size, also because this is bidirectional ==> 256*2 = 512)\n",
    "        out = torch.cat((avg_pool, max_pool), 1)\n",
    "        # fit out to self.out to conduct dimensionality reduction from 512 to 1\n",
    "        out = self.out(out)\n",
    "        # return output\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_loader, model, optimizer, device):\n",
    "    \"\"\"\n",
    "    this is model training for one epoch\n",
    "    data_loader:  this is torch dataloader, just like dataset but in torch and devide into batches\n",
    "    model : lstm\n",
    "    optimizer : torch optimizer : adam\n",
    "    device:  cuda or cpu\n",
    "    \"\"\"\n",
    "    # set model to training mode\n",
    "    model.train()\n",
    "    # go through batches of data in data loader\n",
    "    for data in data_loader:\n",
    "        reviews = data['review']\n",
    "        targets = data['target']\n",
    "        # move the data to device that we want to use\n",
    "        reviews = reviews.to(device, dtype = torch.long)\n",
    "        targets = targets.to(device, dtype = torch.float)\n",
    "        # clear the gradient\n",
    "        optimizer.zero_grad()\n",
    "        # make prediction from model\n",
    "        predictions = model(reviews)\n",
    "        # caculate the losses\n",
    "        loss = nn.BCEWithLogitsLoss()(predictions, targets.view(-1,1))\n",
    "        # backprob\n",
    "        loss.backward()\n",
    "        #single optimization step\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_loader, model, device):\n",
    "    final_predictions = []\n",
    "    final_targets = []\n",
    "    model.eval()\n",
    "    # turn off gradient calculation\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            reviews = data['review']\n",
    "            targets = data['target']\n",
    "            reviews = reviews.to(device, dtype = torch.long)\n",
    "            targets = targets.to(device, dtype=torch.float)\n",
    "            # make prediction\n",
    "            predictions = model(reviews)\n",
    "            # move prediction and target to cpu\n",
    "            predictions = predictions.cpu().numpy().tolist()\n",
    "            targets = data['target'].cpu().numpy().tolist()\n",
    "            # add predictions to final_prediction\n",
    "            final_predictions.extend(predictions)\n",
    "            final_targets.extend(targets)\n",
    "    return final_predictions, final_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 128 # maximum length for a sentence\n",
    "TRAIN_BATCH_SIZE = 16\n",
    "VALID_BATCH_SIZE = 8\n",
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(word_index, embedding_dict=None, dim=300):\n",
    "    \"\"\"\n",
    "     this function create the embedding matrix save in numpy array\n",
    "    :param word_index: a dictionary with word: index_value\n",
    "    :param embedding_dict: a dict with word embedding\n",
    "    :d_model: the dimension of word pretrained embedding\n",
    "    :return a numpy array with embedding vectors for all known words\n",
    "    \"\"\"\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, dim))\n",
    "    ## loop over all the words\n",
    "    for word, index in word_index.items():\n",
    "        if word in embedding_dict:\n",
    "            embedding_matrix[index] = embedding_dict[word]\n",
    "    return embedding_matrix\n",
    "\n",
    "# embedding_dict['word'] = vector\n",
    "# word_index['word'] = index\n",
    "# embedding_matrix[index] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['uzun süredir beyazperde ye konmasını bekledğim. kesinlikle izlenmesi gereken bir film. hatırladğım kadarıyla oyuncuların çoğu halkın içinden seçilmiştir. çok etkileyici...\\n',\n",
       " 'tek kelime: B A Ş Y A P I T .hanginiz dünyadaki en iyi filmleri konusurken bu filmi es geciyonuz ki....\\n',\n",
       " 'çok keyifli bi filmdi izlerken hiç sıkılmadım.10/10 filmden alıntı’eğer bir toplantı sensiz başlamıyorsa gitmeye değerdir’\\n',\n",
       " 'sürükleyici ve kaçırılmaması gereken bir film. 9/10\\n',\n",
       " 'filmden o kadar etkilenmiştim ki...özellikle özgürlük uğruna savaşan william wallace’in işkence edilerek idam edilmesi...ve sonunda özgürlük diye haykırışı...halan unutamıyorum...\\n']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindf['review'].values.tolist()[:5] # list of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Tokenization\n",
    "# use tf.keras for tokenization,  \n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(traindf['review'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bir': 1,\n",
       " 'film': 2,\n",
       " 'bu': 3,\n",
       " 've': 4,\n",
       " 'çok': 5,\n",
       " 'filmi': 6,\n",
       " 'en': 7,\n",
       " 'ama': 8,\n",
       " '10': 9,\n",
       " 'de': 10,\n",
       " 'iyi': 11,\n",
       " 'kadar': 12,\n",
       " 'da': 13,\n",
       " 'filmin': 14,\n",
       " 'bi': 15,\n",
       " 'daha': 16,\n",
       " 'için': 17,\n",
       " 'güzel': 18,\n",
       " 'ne': 19,\n",
       " 'o': 20,\n",
       " 'bence': 21,\n",
       " 'gibi': 22,\n",
       " 'filmde': 23,\n",
       " 'ben': 24,\n",
       " 'her': 25,\n",
       " 'hiç': 26,\n",
       " 'yok': 27,\n",
       " 'ki': 28,\n",
       " 'kötü': 29,\n",
       " 'var': 30,\n",
       " 'gerçekten': 31,\n",
       " 'bile': 32,\n",
       " 'olarak': 33,\n",
       " 'sonra': 34,\n",
       " 'kesinlikle': 35,\n",
       " 'değil': 36,\n",
       " 'ilk': 37,\n",
       " 'harika': 38,\n",
       " 'mükemmel': 39,\n",
       " 'filme': 40,\n",
       " 'ile': 41,\n",
       " 'tek': 42,\n",
       " 'olan': 43,\n",
       " 'biri': 44,\n",
       " 'son': 45,\n",
       " 'zaten': 46,\n",
       " 'sinema': 47,\n",
       " 'zaman': 48,\n",
       " 'ya': 49,\n",
       " 'büyük': 50,\n",
       " 'şey': 51,\n",
       " 'izledim': 52,\n",
       " 'sadece': 53,\n",
       " 'böyle': 54,\n",
       " '2': 55,\n",
       " 'tam': 56,\n",
       " '9': 57,\n",
       " 'diye': 58,\n",
       " 'izlediğim': 59,\n",
       " 'filmlerden': 60,\n",
       " 'nasıl': 61,\n",
       " 'izleyin': 62,\n",
       " 'senaryo': 63,\n",
       " 'yani': 64,\n",
       " 'tavsiye': 65,\n",
       " 'muhteşem': 66,\n",
       " 'süper': 67,\n",
       " 'fazla': 68,\n",
       " 'olmuş': 69,\n",
       " 'benim': 70,\n",
       " 'bana': 71,\n",
       " '1': 72,\n",
       " '3': 73,\n",
       " 'cok': 74,\n",
       " 'olduğunu': 75,\n",
       " 'yine': 76,\n",
       " 'mutlaka': 77,\n",
       " 'filmdi': 78,\n",
       " 'gereken': 79,\n",
       " 'filmden': 80,\n",
       " 'biraz': 81,\n",
       " 'müthiş': 82,\n",
       " 'ise': 83,\n",
       " 'başarılı': 84,\n",
       " 'konu': 85,\n",
       " 'göre': 86,\n",
       " 'oyunculuk': 87,\n",
       " 'olduğu': 88,\n",
       " 'başyapıt': 89,\n",
       " 'özellikle': 90,\n",
       " 'önce': 91,\n",
       " 'beni': 92,\n",
       " 'ayrıca': 93,\n",
       " 'aynı': 94,\n",
       " '5': 95,\n",
       " 'sıkıcı': 96,\n",
       " '4': 97,\n",
       " 'oyuncu': 98,\n",
       " 'yönetmen': 99,\n",
       " 'başka': 100,\n",
       " 'ancak': 101,\n",
       " 'gerek': 102,\n",
       " 'çünkü': 103,\n",
       " 'farklı': 104,\n",
       " 'filmleri': 105,\n",
       " 'adam': 106,\n",
       " 'tüm': 107,\n",
       " 'belki': 108,\n",
       " 'filmlerinden': 109,\n",
       " 'iki': 110,\n",
       " 'filmler': 111,\n",
       " 'korku': 112,\n",
       " 'aksiyon': 113,\n",
       " 'rağmen': 114,\n",
       " 'etkileyici': 115,\n",
       " 'pek': 116,\n",
       " 'olur': 117,\n",
       " 'mi': 118,\n",
       " 'hala': 119,\n",
       " 'hem': 120,\n",
       " 'konusu': 121,\n",
       " 'puan': 122,\n",
       " 'uzun': 123,\n",
       " 'oldu': 124,\n",
       " 'tekrar': 125,\n",
       " 'gerilim': 126,\n",
       " 'fakat': 127,\n",
       " 'sonunda': 128,\n",
       " 'izlerken': 129,\n",
       " 'gerçek': 130,\n",
       " 'şekilde': 131,\n",
       " 'az': 132,\n",
       " 'saçma': 133,\n",
       " 'artık': 134,\n",
       " 'kurgu': 135,\n",
       " 'öyle': 136,\n",
       " 'içinde': 137,\n",
       " 'insan': 138,\n",
       " 'neden': 139,\n",
       " 'kez': 140,\n",
       " 'aslında': 141,\n",
       " 'hayal': 142,\n",
       " 'bunu': 143,\n",
       " 'yer': 144,\n",
       " 'olsa': 145,\n",
       " 'olması': 146,\n",
       " 'yapım': 147,\n",
       " 'ederim': 148,\n",
       " 'savaş': 149,\n",
       " 'hayatımda': 150,\n",
       " 'komedi': 151,\n",
       " 'diğer': 152,\n",
       " 'sahnesi': 153,\n",
       " 'şu': 154,\n",
       " 'klasik': 155,\n",
       " 'the': 156,\n",
       " 'bazı': 157,\n",
       " 'sonu': 158,\n",
       " 'hatta': 159,\n",
       " 'kendi': 160,\n",
       " 'berbat': 161,\n",
       " 'in': 162,\n",
       " 'olsun': 163,\n",
       " 'izlemek': 164,\n",
       " 'kelimeyle': 165,\n",
       " 'işte': 166,\n",
       " 'onun': 167,\n",
       " 'yeni': 168,\n",
       " 'yorum': 169,\n",
       " 'insanı': 170,\n",
       " 'arkadaşlar': 171,\n",
       " 'inanılmaz': 172,\n",
       " 'mı': 173,\n",
       " 'sahne': 174,\n",
       " 'oldukça': 175,\n",
       " 'tom': 176,\n",
       " 'devam': 177,\n",
       " 'hep': 178,\n",
       " 'ortaya': 179,\n",
       " 'önemli': 180,\n",
       " 'aşk': 181,\n",
       " 'sanki': 182,\n",
       " 'arasında': 183,\n",
       " 'oyuncular': 184,\n",
       " 'bunun': 185,\n",
       " 'izlenmesi': 186,\n",
       " 'varsa': 187,\n",
       " 'hele': 188,\n",
       " 'sonuna': 189,\n",
       " 'sahneleri': 190,\n",
       " 'yazık': 191,\n",
       " '8': 192,\n",
       " 'kere': 193,\n",
       " 'tabi': 194,\n",
       " 'zor': 195,\n",
       " 'sanırım': 196,\n",
       " 'al': 197,\n",
       " 'hiçbir': 198,\n",
       " 'eğer': 199,\n",
       " 'ediyorum': 200,\n",
       " 'derim': 201,\n",
       " 'sahneler': 202,\n",
       " 'oyunculuklar': 203,\n",
       " 'tarihinin': 204,\n",
       " 'bütün': 205,\n",
       " 'komik': 206,\n",
       " 'filmdeki': 207,\n",
       " 'beğenmedim': 208,\n",
       " 'puanı': 209,\n",
       " 'insanın': 210,\n",
       " 'bende': 211,\n",
       " 'üzerine': 212,\n",
       " 'dolu': 213,\n",
       " 'ayrı': 214,\n",
       " 'serinin': 215,\n",
       " 'üzerinden': 216,\n",
       " 'gayet': 217,\n",
       " 'ın': 218,\n",
       " 'düşünüyorum': 219,\n",
       " 'basit': 220,\n",
       " 'gereksiz': 221,\n",
       " 'sinemada': 222,\n",
       " 'sahip': 223,\n",
       " 'herkes': 224,\n",
       " 'diyebilirim': 225,\n",
       " 'sıradan': 226,\n",
       " 'yıl': 227,\n",
       " 'doğru': 228,\n",
       " 'oscar': 229,\n",
       " 'eden': 230,\n",
       " 'arada': 231,\n",
       " 'hak': 232,\n",
       " 'şimdi': 233,\n",
       " 'kaliteli': 234,\n",
       " 'olabilir': 235,\n",
       " 'kendini': 236,\n",
       " 'eğlenceli': 237,\n",
       " 'lazım': 238,\n",
       " 'onu': 239,\n",
       " 'vasat': 240,\n",
       " 'olmayan': 241,\n",
       " 'güzeldi': 242,\n",
       " 'kaybı': 243,\n",
       " 'derece': 244,\n",
       " 'amerikan': 245,\n",
       " 'dram': 246,\n",
       " 'senaryosu': 247,\n",
       " 'birşey': 248,\n",
       " 'geldi': 249,\n",
       " 'bişey': 250,\n",
       " 'a': 251,\n",
       " 'izlemeye': 252,\n",
       " 'usta': 253,\n",
       " 'd': 254,\n",
       " 'kaç': 255,\n",
       " 'yapılmış': 256,\n",
       " 'herkese': 257,\n",
       " 'boş': 258,\n",
       " '6': 259,\n",
       " 'oyunculuğu': 260,\n",
       " 'robert': 261,\n",
       " 'belli': 262,\n",
       " 'merak': 263,\n",
       " 'geçen': 264,\n",
       " 'dışında': 265,\n",
       " 'vardı': 266,\n",
       " 'olmasına': 267,\n",
       " 'herkesin': 268,\n",
       " 'hakkında': 269,\n",
       " 'izlenmeli': 270,\n",
       " 'kusursuz': 271,\n",
       " 'tamamen': 272,\n",
       " 'filmle': 273,\n",
       " 'boyunca': 274,\n",
       " 'hoş': 275,\n",
       " 'vakit': 276,\n",
       " 'görsel': 277,\n",
       " 'ediyor': 278,\n",
       " 'sizi': 279,\n",
       " 'falan': 280,\n",
       " 'hemen': 281,\n",
       " 'oluyor': 282,\n",
       " 'i': 283,\n",
       " 'birisi': 284,\n",
       " 'resmen': 285,\n",
       " 'şeyler': 286,\n",
       " '7': 287,\n",
       " 'beğendim': 288,\n",
       " 'i̇lk': 289,\n",
       " 'hayran': 290,\n",
       " 'küçük': 291,\n",
       " 'ilginç': 292,\n",
       " 'filmlerini': 293,\n",
       " 'izledikten': 294,\n",
       " 'müzikleri': 295,\n",
       " 'adamın': 296,\n",
       " 'filmini': 297,\n",
       " 'yoksa': 298,\n",
       " 'buna': 299,\n",
       " 'an': 300,\n",
       " 'para': 301,\n",
       " 'türk': 302,\n",
       " 'değer': 303,\n",
       " 'iş': 304,\n",
       " 'niro': 305,\n",
       " 'eski': 306,\n",
       " 'ii': 307,\n",
       " 'john': 308,\n",
       " 'veya': 309,\n",
       " 'filmdir': 310,\n",
       " 'saat': 311,\n",
       " 'asla': 312,\n",
       " 'bır': 313,\n",
       " 'keşke': 314,\n",
       " 'bi̇r': 315,\n",
       " 'pacino': 316,\n",
       " 'e': 317,\n",
       " 'geçmiş': 318,\n",
       " 'jim': 319,\n",
       " 'fılm': 320,\n",
       " 'acaba': 321,\n",
       " 'defa': 322,\n",
       " 'defalarca': 323,\n",
       " 'yönetmenin': 324,\n",
       " 'ötesi': 325,\n",
       " 'dünya': 326,\n",
       " 'uzak': 327,\n",
       " 'adeta': 328,\n",
       " 'yapıt': 329,\n",
       " 'evet': 330,\n",
       " 'bilim': 331,\n",
       " 'açıkçası': 332,\n",
       " 'kült': 333,\n",
       " 'gelmiş': 334,\n",
       " 'ona': 335,\n",
       " 'sağlam': 336,\n",
       " 'romantik': 337,\n",
       " 'geliyor': 338,\n",
       " 'i̇yi': 339,\n",
       " 'iyiydi': 340,\n",
       " 'herşey': 341,\n",
       " 'tarafından': 342,\n",
       " 'hikaye': 343,\n",
       " 'olmak': 344,\n",
       " 'demek': 345,\n",
       " 'müzik': 346,\n",
       " 'insanların': 347,\n",
       " 'yada': 348,\n",
       " 'hepsi': 349,\n",
       " 'sürükleyici': 350,\n",
       " 'birlikte': 351,\n",
       " 'oynamış': 352,\n",
       " 'oldum': 353,\n",
       " 'ikinci': 354,\n",
       " 'jack': 355,\n",
       " 'birçok': 356,\n",
       " 'yerine': 357,\n",
       " 'filmlerin': 358,\n",
       " 'olacak': 359,\n",
       " 'müzikler': 360,\n",
       " 'tim': 361,\n",
       " 'iyisi': 362,\n",
       " 'duygusal': 363,\n",
       " 'yapmış': 364,\n",
       " 'anlamadım': 365,\n",
       " 'kurgusu': 366,\n",
       " 'baştan': 367,\n",
       " 'anlatan': 368,\n",
       " 'aldığı': 369,\n",
       " 'nin': 370,\n",
       " 'kadın': 371,\n",
       " 'filminde': 372,\n",
       " 'bizim': 373,\n",
       " 'ağır': 374,\n",
       " 'sonunu': 375,\n",
       " 'ilgili': 376,\n",
       " 'içine': 377,\n",
       " 'yaptığı': 378,\n",
       " 'kendine': 379,\n",
       " 'çekilmiş': 380,\n",
       " 'herhalde': 381,\n",
       " 'izlemeyen': 382,\n",
       " 'bize': 383,\n",
       " 'performansı': 384,\n",
       " 'bundan': 385,\n",
       " 'çocuk': 386,\n",
       " 'yapımı': 387,\n",
       " 'animasyon': 388,\n",
       " 'sona': 389,\n",
       " 'yüksek': 390,\n",
       " 'hikayesi': 391,\n",
       " 'yüzden': 392,\n",
       " 'size': 393,\n",
       " 'zamanda': 394,\n",
       " 'kısa': 395,\n",
       " 'gün': 396,\n",
       " 'olağanüstü': 397,\n",
       " 'diyorum': 398,\n",
       " 'izlemiştim': 399,\n",
       " 'insanlar': 400,\n",
       " 'yere': 401,\n",
       " 'alan': 402,\n",
       " 'gelen': 403,\n",
       " 'kırıklığı': 404,\n",
       " 'sırf': 405,\n",
       " 'filmlerinin': 406,\n",
       " 'izlemeyin': 407,\n",
       " 'sonuç': 408,\n",
       " 'yanında': 409,\n",
       " 'sinemaya': 410,\n",
       " 'pişman': 411,\n",
       " 'göz': 412,\n",
       " 'baş': 413,\n",
       " 'yanı': 414,\n",
       " 'efsane': 415,\n",
       " 'karşı': 416,\n",
       " 'çizgi': 417,\n",
       " 'izlediğimde': 418,\n",
       " 'brad': 419,\n",
       " 'sevdiğim': 420,\n",
       " 'üç': 421,\n",
       " 'çoğu': 422,\n",
       " 'vardır': 423,\n",
       " 'vs': 424,\n",
       " 'tarz': 425,\n",
       " 'yıllar': 426,\n",
       " 'rol': 427,\n",
       " 'şiddetle': 428,\n",
       " 'dikkat': 429,\n",
       " 'hanks': 430,\n",
       " 'siz': 431,\n",
       " \"'\": 432,\n",
       " 'final': 433,\n",
       " 'kim': 434,\n",
       " 'yinede': 435,\n",
       " 'almış': 436,\n",
       " 'guzel': 437,\n",
       " 'karakter': 438,\n",
       " 'performans': 439,\n",
       " 'david': 440,\n",
       " 'hayat': 441,\n",
       " 'kelime': 442,\n",
       " 'değildi': 443,\n",
       " 'izlemesi': 444,\n",
       " 'biz': 445,\n",
       " 'veren': 446,\n",
       " 'sanat': 447,\n",
       " 'kubrick': 448,\n",
       " 'altında': 449,\n",
       " 'filim': 450,\n",
       " 'oyuncuların': 451,\n",
       " 'un': 452,\n",
       " 'zevk': 453,\n",
       " 'görmek': 454,\n",
       " 'yanlış': 455,\n",
       " 'söz': 456,\n",
       " 'anda': 457,\n",
       " 'dedim': 458,\n",
       " 'buldum': 459,\n",
       " 'yapan': 460,\n",
       " 'sakın': 461,\n",
       " 'bilmiyorum': 462,\n",
       " 'anlatıyor': 463,\n",
       " 'tabii': 464,\n",
       " 'bol': 465,\n",
       " 'gerçekçi': 466,\n",
       " 'fazlasıyla': 467,\n",
       " 'olay': 468,\n",
       " 'dvd': 469,\n",
       " 'kimse': 470,\n",
       " 'yapılan': 471,\n",
       " 'filmlere': 472,\n",
       " 'şunu': 473,\n",
       " 'tv': 474,\n",
       " 'tür': 475,\n",
       " 'başına': 476,\n",
       " 'türlü': 477,\n",
       " 'yakın': 478,\n",
       " 'tamam': 479,\n",
       " 'karakterler': 480,\n",
       " 'kendimi': 481,\n",
       " 'genel': 482,\n",
       " 'süre': 483,\n",
       " 'tahmin': 484,\n",
       " 'filmlerinde': 485,\n",
       " 'verdiği': 486,\n",
       " 'kişi': 487,\n",
       " 'nun': 488,\n",
       " 'yaa': 489,\n",
       " 'anlamıyla': 490,\n",
       " 'den': 491,\n",
       " 'verdim': 492,\n",
       " 'sene': 493,\n",
       " 'mu': 494,\n",
       " 'kız': 495,\n",
       " 'yi': 496,\n",
       " 'ana': 497,\n",
       " 'seyirler': 498,\n",
       " 'neyse': 499,\n",
       " 'işi': 500,\n",
       " 'unutulmaz': 501,\n",
       " 'harikaydı': 502,\n",
       " 'of': 503,\n",
       " 'görüntü': 504,\n",
       " 'değişik': 505,\n",
       " 'macera': 506,\n",
       " 'anlamsız': 507,\n",
       " 'western': 508,\n",
       " 'keyif': 509,\n",
       " 'özel': 510,\n",
       " 'valla': 511,\n",
       " 'edward': 512,\n",
       " 'filminden': 513,\n",
       " 'doğrusu': 514,\n",
       " 'olmaz': 515,\n",
       " 'efektler': 516,\n",
       " 'sapan': 517,\n",
       " 'tarzı': 518,\n",
       " 'sahnede': 519,\n",
       " 'dört': 520,\n",
       " 'kalmış': 521,\n",
       " 'michael': 522,\n",
       " 'amp': 523,\n",
       " 'kolay': 524,\n",
       " 'lütfen': 525,\n",
       " 'neredeyse': 526,\n",
       " 'tane': 527,\n",
       " 'yapmak': 528,\n",
       " 'mümkün': 529,\n",
       " 'kamera': 530,\n",
       " 'kısacası': 531,\n",
       " 'hakkını': 532,\n",
       " 'sen': 533,\n",
       " 'dünyanın': 534,\n",
       " 'etmek': 535,\n",
       " 'baya': 536,\n",
       " 'olaylar': 537,\n",
       " 'gerekir': 538,\n",
       " 'beraber': 539,\n",
       " 'izleyip': 540,\n",
       " 'izlesin': 541,\n",
       " 'veriyor': 542,\n",
       " 'yeniden': 543,\n",
       " 'açısından': 544,\n",
       " 'matrix': 545,\n",
       " 'olamaz': 546,\n",
       " 'dahi': 547,\n",
       " 'birkaç': 548,\n",
       " 'james': 549,\n",
       " 'fi̇lm': 550,\n",
       " 'sıkıldım': 551,\n",
       " 'sürekli': 552,\n",
       " 'yeri': 553,\n",
       " 'yılında': 554,\n",
       " 'filmlerine': 555,\n",
       " 'ödülü': 556,\n",
       " 'gercekten': 557,\n",
       " 'kaçırmayın': 558,\n",
       " 'biriydi': 559,\n",
       " 'tabiki': 560,\n",
       " 'olurdu': 561,\n",
       " 'boşa': 562,\n",
       " 'sıkılmadan': 563,\n",
       " 'açık': 564,\n",
       " 'dan': 565,\n",
       " 'bazen': 566,\n",
       " 'genç': 567,\n",
       " 'söze': 568,\n",
       " 'üzerinde': 569,\n",
       " 'bugün': 570,\n",
       " 'türünün': 571,\n",
       " 'yavaş': 572,\n",
       " 'adına': 573,\n",
       " 'neler': 574,\n",
       " 'yoktu': 575,\n",
       " 'karar': 576,\n",
       " 'hollywood': 577,\n",
       " 'istiyorum': 578,\n",
       " 'erkek': 579,\n",
       " 'maalesef': 580,\n",
       " 'arasına': 581,\n",
       " 'geri': 582,\n",
       " 'başında': 583,\n",
       " 'şahane': 584,\n",
       " 'bı': 585,\n",
       " 'izlemeden': 586,\n",
       " 'kırıklığına': 587,\n",
       " 'izlemeli': 588,\n",
       " 'fantastik': 589,\n",
       " 'izlenir': 590,\n",
       " 'karakteri': 591,\n",
       " 'bilimkurgu': 592,\n",
       " 'kabul': 593,\n",
       " 'pitt': 594,\n",
       " 'sean': 595,\n",
       " 'çıkmış': 596,\n",
       " 'belkide': 597,\n",
       " 'kalan': 598,\n",
       " 'ondan': 599,\n",
       " 'olmamış': 600,\n",
       " 'ses': 601,\n",
       " 'biridir': 602,\n",
       " 'hangi': 603,\n",
       " 'johnny': 604,\n",
       " 'carrey': 605,\n",
       " 'bunlar': 606,\n",
       " '100': 607,\n",
       " 'diyaloglar': 608,\n",
       " 'alıp': 609,\n",
       " 'aldım': 610,\n",
       " 'i̇zlediğim': 611,\n",
       " 'kesin': 612,\n",
       " 'be': 613,\n",
       " 'müziği': 614,\n",
       " '20': 615,\n",
       " 'burada': 616,\n",
       " 'sinemanın': 617,\n",
       " 'finali': 618,\n",
       " 'kevin': 619,\n",
       " 'depp': 620,\n",
       " 'hani': 621,\n",
       " 'hoşuma': 622,\n",
       " 'on': 623,\n",
       " 'mesaj': 624,\n",
       " 'tanesi': 625,\n",
       " 'azından': 626,\n",
       " 'ilgi': 627,\n",
       " 'numara': 628,\n",
       " 'aday': 629,\n",
       " 'izlerim': 630,\n",
       " 'baba': 631,\n",
       " 'şeyi': 632,\n",
       " 'duygu': 633,\n",
       " 'yeter': 634,\n",
       " 'not': 635,\n",
       " 'vermiş': 636,\n",
       " 'ı': 637,\n",
       " 'bruce': 638,\n",
       " 'yerde': 639,\n",
       " 'dövüş': 640,\n",
       " 'derecede': 641,\n",
       " 'insana': 642,\n",
       " 'birde': 643,\n",
       " 'yönetmeni': 644,\n",
       " 'seyrettim': 645,\n",
       " 'crowe': 646,\n",
       " 'tarantino': 647,\n",
       " 'roman': 648,\n",
       " 'izleme': 649,\n",
       " 'adamı': 650,\n",
       " 'üst': 651,\n",
       " 'zayıf': 652,\n",
       " 'yapıyor': 653,\n",
       " 'yerinde': 654,\n",
       " 'heyecan': 655,\n",
       " 'hayata': 656,\n",
       " 'filmidir': 657,\n",
       " 'dk': 658,\n",
       " 'nadir': 659,\n",
       " 'kötüydü': 660,\n",
       " 'spielberg': 661,\n",
       " 'önüne': 662,\n",
       " 'bizi': 663,\n",
       " 'çıkarmış': 664,\n",
       " 'dolayı': 665,\n",
       " 'icin': 666,\n",
       " 'izlemedim': 667,\n",
       " 'değilim': 668,\n",
       " 'çıkan': 669,\n",
       " 'sonraki': 670,\n",
       " 'filmlerde': 671,\n",
       " 'yol': 672,\n",
       " 'öncelikle': 673,\n",
       " 'kadrosu': 674,\n",
       " 'kat': 675,\n",
       " 'rezalet': 676,\n",
       " 'hemde': 677,\n",
       " 'izlenebilir': 678,\n",
       " 'geç': 679,\n",
       " 'degil': 680,\n",
       " 'psikolojik': 681,\n",
       " 'budur': 682,\n",
       " 'şimdiye': 683,\n",
       " 'sonuçta': 684,\n",
       " 'olmasa': 685,\n",
       " 'dair': 686,\n",
       " 'çarpıcı': 687,\n",
       " 'norton': 688,\n",
       " 'başta': 689,\n",
       " 'adı': 690,\n",
       " 'süperdi': 691,\n",
       " 'gelir': 692,\n",
       " 'efektleri': 693,\n",
       " 'niye': 694,\n",
       " 'oyunculukları': 695,\n",
       " 'oynadığı': 696,\n",
       " 'kaldım': 697,\n",
       " 'bide': 698,\n",
       " 'eminim': 699,\n",
       " 'anlamak': 700,\n",
       " 'kendisi': 701,\n",
       " 'başından': 702,\n",
       " 'kara': 703,\n",
       " 'gelince': 704,\n",
       " 'zamana': 705,\n",
       " 'aklıma': 706,\n",
       " 'işlenmiş': 707,\n",
       " 'müzikal': 708,\n",
       " 'anlam': 709,\n",
       " 'ciddi': 710,\n",
       " 'dakika': 711,\n",
       " 'sinemasının': 712,\n",
       " 'dün': 713,\n",
       " 'tarihi': 714,\n",
       " 'mutlu': 715,\n",
       " 'hakediyor': 716,\n",
       " 'fena': 717,\n",
       " 'açıdan': 718,\n",
       " 'beyaz': 719,\n",
       " 'sergilemiş': 720,\n",
       " 'etmiş': 721,\n",
       " 'orjinal': 722,\n",
       " 'elde': 723,\n",
       " 'olayı': 724,\n",
       " 'çıkıyor': 725,\n",
       " 'favori': 726,\n",
       " 'normal': 727,\n",
       " 'etkisi': 728,\n",
       " '11': 729,\n",
       " 'abartılı': 730,\n",
       " 'olduğum': 731,\n",
       " 'ders': 732,\n",
       " 'yerini': 733,\n",
       " 'başyapıtı': 734,\n",
       " 'ara': 735,\n",
       " 'nicholson': 736,\n",
       " 'diken': 737,\n",
       " 'izlemiş': 738,\n",
       " 'keyifli': 739,\n",
       " 'hayatı': 740,\n",
       " 'herşeyiyle': 741,\n",
       " 'vermek': 742,\n",
       " 'anlamda': 743,\n",
       " 'izlemenizi': 744,\n",
       " 'hayatın': 745,\n",
       " 'woody': 746,\n",
       " 'gece': 747,\n",
       " 'anladım': 748,\n",
       " 'burton': 749,\n",
       " 'gördüm': 750,\n",
       " 'mantık': 751,\n",
       " 'seri': 752,\n",
       " 'heralde': 753,\n",
       " 'filmiydi': 754,\n",
       " 'olmalı': 755,\n",
       " 'hale': 756,\n",
       " 'anlatım': 757,\n",
       " 'anlamıyorum': 758,\n",
       " 'mesela': 759,\n",
       " 'gene': 760,\n",
       " 'idi': 761,\n",
       " 'yönetmenlik': 762,\n",
       " 'mel': 763,\n",
       " 'klişe': 764,\n",
       " 'oldugunu': 765,\n",
       " 'adamlar': 766,\n",
       " 'burda': 767,\n",
       " 'etkileyen': 768,\n",
       " 'filminin': 769,\n",
       " 'yorumları': 770,\n",
       " 'eder': 771,\n",
       " 'flim': 772,\n",
       " 's': 773,\n",
       " 'oyuncuları': 774,\n",
       " 'başarısız': 775,\n",
       " 'üstelik': 776,\n",
       " 'üzere': 777,\n",
       " 'boşuna': 778,\n",
       " 'vizyona': 779,\n",
       " 'kendime': 780,\n",
       " 'gittim': 781,\n",
       " 'yorumlara': 782,\n",
       " 'izleyince': 783,\n",
       " 'arkadaş': 784,\n",
       " 'ettiği': 785,\n",
       " 'izlemeyenler': 786,\n",
       " 'güçlü': 787,\n",
       " '15': 788,\n",
       " 'genelde': 789,\n",
       " 'rolü': 790,\n",
       " 'yalnız': 791,\n",
       " 'herşeyi': 792,\n",
       " 'nefret': 793,\n",
       " 'arasındaki': 794,\n",
       " 'ele': 795,\n",
       " 'çekici': 796,\n",
       " 'mafya': 797,\n",
       " 'gösteriyor': 798,\n",
       " 'izlemediyseniz': 799,\n",
       " 'sey': 800,\n",
       " 'eksik': 801,\n",
       " 'bakış': 802,\n",
       " 'görmedim': 803,\n",
       " 'sayesinde': 804,\n",
       " 'aldı': 805,\n",
       " '12': 806,\n",
       " 'konusunda': 807,\n",
       " 'savaşın': 808,\n",
       " 'ince': 809,\n",
       " 'örnek': 810,\n",
       " 'aile': 811,\n",
       " 'oscarı': 812,\n",
       " 'galiba': 813,\n",
       " 'cage': 814,\n",
       " 'anlatmak': 815,\n",
       " 'zamanlarda': 816,\n",
       " 'sürü': 817,\n",
       " 'çekilen': 818,\n",
       " 'şaheser': 819,\n",
       " 'alıyor': 820,\n",
       " 'saygı': 821,\n",
       " 'görünce': 822,\n",
       " 'u': 823,\n",
       " 'yapmaya': 824,\n",
       " 'efekt': 825,\n",
       " 'değmez': 826,\n",
       " 'kotu': 827,\n",
       " 'gitti': 828,\n",
       " 'üstüne': 829,\n",
       " 'ettim': 830,\n",
       " 'etmem': 831,\n",
       " 'i̇şte': 832,\n",
       " 'izleyiciyi': 833,\n",
       " 'hayatımın': 834,\n",
       " 'jean': 835,\n",
       " 'zamanki': 836,\n",
       " 'ufak': 837,\n",
       " 'konuyu': 838,\n",
       " 'zamanların': 839,\n",
       " 'benden': 840,\n",
       " 'kafa': 841,\n",
       " 'edici': 842,\n",
       " 'savaşı': 843,\n",
       " 'adlı': 844,\n",
       " 'aşırı': 845,\n",
       " 'değildir': 846,\n",
       " 'içerisinde': 847,\n",
       " 'ortada': 848,\n",
       " 'aşık': 849,\n",
       " 'çeken': 850,\n",
       " 'katil': 851,\n",
       " 'nedir': 852,\n",
       " 'olabilirdi': 853,\n",
       " 'iç': 854,\n",
       " 'izledigim': 855,\n",
       " 'izleyen': 856,\n",
       " 'hitchcock': 857,\n",
       " 'stanley': 858,\n",
       " 'çıktı': 859,\n",
       " 'dediği': 860,\n",
       " 'abd': 861,\n",
       " 'akıcı': 862,\n",
       " 'inanın': 863,\n",
       " 'mükemmeldi': 864,\n",
       " 'görsellik': 865,\n",
       " 'derin': 866,\n",
       " 'olmadığını': 867,\n",
       " 'yapmışlar': 868,\n",
       " 'insanları': 869,\n",
       " 'durum': 870,\n",
       " 'bulamıyorum': 871,\n",
       " 'su': 872,\n",
       " 'umarım': 873,\n",
       " 'kişinin': 874,\n",
       " 'sahnesinde': 875,\n",
       " 'i̇zlerken': 876,\n",
       " 'takip': 877,\n",
       " 'çekim': 878,\n",
       " 'olunca': 879,\n",
       " 'hakikaten': 880,\n",
       " 'seyirciyi': 881,\n",
       " 'hayatta': 882,\n",
       " 'gerçektende': 883,\n",
       " 'zamanlar': 884,\n",
       " 'başrol': 885,\n",
       " 'bulamadım': 886,\n",
       " 'olursa': 887,\n",
       " 'onlar': 888,\n",
       " 'ünlü': 889,\n",
       " 'gerekiyor': 890,\n",
       " 'we': 891,\n",
       " 'izlenmeye': 892,\n",
       " 'kendisini': 893,\n",
       " 'verilen': 894,\n",
       " 'diyecek': 895,\n",
       " 'olmuştur': 896,\n",
       " 'steven': 897,\n",
       " 'söyleyebilirim': 898,\n",
       " 'haline': 899,\n",
       " 'söylemek': 900,\n",
       " 'beğendiğim': 901,\n",
       " 'filmine': 902,\n",
       " 'hayranı': 903,\n",
       " 'olmadı': 904,\n",
       " 'nedense': 905,\n",
       " 'hava': 906,\n",
       " 'olup': 907,\n",
       " 'şöyle': 908,\n",
       " 'açıkcası': 909,\n",
       " 'çalışan': 910,\n",
       " 't': 911,\n",
       " 'katılıyorum': 912,\n",
       " 'kalıyor': 913,\n",
       " 'milyon': 914,\n",
       " 'hadi': 915,\n",
       " 'tatmin': 916,\n",
       " 'tatlı': 917,\n",
       " 'yaşam': 918,\n",
       " 'olmadığı': 919,\n",
       " 'dramatik': 920,\n",
       " 'oyunu': 921,\n",
       " 'oynayan': 922,\n",
       " 'fiyasko': 923,\n",
       " 'olmus': 924,\n",
       " 'görün': 925,\n",
       " 'hitap': 926,\n",
       " 'nicolas': 927,\n",
       " 'rolünde': 928,\n",
       " 'böle': 929,\n",
       " 'si': 930,\n",
       " 'akıl': 931,\n",
       " 'düşük': 932,\n",
       " 'tarihine': 933,\n",
       " 'birini': 934,\n",
       " 'onların': 935,\n",
       " 'yarım': 936,\n",
       " 'olsada': 937,\n",
       " 'fılmı': 938,\n",
       " 'ha': 939,\n",
       " 'halde': 940,\n",
       " 'giden': 941,\n",
       " 'can': 942,\n",
       " 'seyredin': 943,\n",
       " 'dünyada': 944,\n",
       " 'dörtlük': 945,\n",
       " 'oda': 946,\n",
       " 'bakımından': 947,\n",
       " 'başlıyor': 948,\n",
       " 'gidin': 949,\n",
       " 'scorsese': 950,\n",
       " 'peter': 951,\n",
       " 'deil': 952,\n",
       " 'gidip': 953,\n",
       " 'denli': 954,\n",
       " 'span': 955,\n",
       " 'izlenecek': 956,\n",
       " 'yeterli': 957,\n",
       " 'j': 958,\n",
       " 'russell': 959,\n",
       " 'olumsuz': 960,\n",
       " 'ender': 961,\n",
       " 'garip': 962,\n",
       " 'kadının': 963,\n",
       " 'yoktur': 964,\n",
       " 'dustin': 965,\n",
       " 'sinemadan': 966,\n",
       " 'geçti': 967,\n",
       " 'zekice': 968,\n",
       " 'yıllarda': 969,\n",
       " 'tadı': 970,\n",
       " 'aklımda': 971,\n",
       " 'gelecek': 972,\n",
       " 'dediğim': 973,\n",
       " 'star': 974,\n",
       " 'kitabı': 975,\n",
       " 'istediği': 976,\n",
       " '30': 977,\n",
       " 'kendinizi': 978,\n",
       " 'şaşırtıcı': 979,\n",
       " 'biçimde': 980,\n",
       " 'cruise': 981,\n",
       " 'geçiyor': 982,\n",
       " 'beklediğim': 983,\n",
       " 'bayıldım': 984,\n",
       " 'seyrettiğim': 985,\n",
       " 'forrest': 986,\n",
       " 'cidden': 987,\n",
       " 'nerde': 988,\n",
       " 'ölüm': 989,\n",
       " 'martin': 990,\n",
       " 'rahatlıkla': 991,\n",
       " 'evde': 992,\n",
       " 'sefer': 993,\n",
       " 'mesajlar': 994,\n",
       " 'oldugu': 995,\n",
       " 'kahraman': 996,\n",
       " 'die': 997,\n",
       " 'ön': 998,\n",
       " 'iğrenç': 999,\n",
       " 'anlamlı': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0 is reserved for padding\n",
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load embeddings\n",
      "training model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:08<00:32,  8.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD:0, epoch: 0, accuracy_score: 0.8814608480346642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:16<00:25,  8.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD:0, epoch: 1, accuracy_score: 0.8879603837821108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [00:26<00:17,  8.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD:0, epoch: 2, accuracy_score: 0.8796038378211081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [00:34<00:08,  8.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD:0, epoch: 3, accuracy_score: 0.887341380377592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:42<00:00,  8.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD:0, epoch: 4, accuracy_score: 0.8947694212318168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:08<00:34,  8.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD:1, epoch: 0, accuracy_score: 0.8882698854843701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:16<00:25,  8.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD:1, epoch: 1, accuracy_score: 0.8947694212318168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [00:24<00:16,  8.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD:1, epoch: 2, accuracy_score: 0.8907458991024451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [00:32<00:08,  8.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD:1, epoch: 3, accuracy_score: 0.8938409161250387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:40<00:00,  8.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD:1, epoch: 4, accuracy_score: 0.8876508820798514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:07<00:31,  7.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD:2, epoch: 0, accuracy_score: 0.8867223769730733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:16<00:24,  8.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD:2, epoch: 1, accuracy_score: 0.8891983905911482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [00:24<00:16,  8.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD:2, epoch: 2, accuracy_score: 0.887341380377592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [00:31<00:07,  7.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD:2, epoch: 3, accuracy_score: 0.8879603837821108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:39<00:00,  7.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD:2, epoch: 4, accuracy_score: 0.8861033735685546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:07<00:29,  7.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD:3, epoch: 0, accuracy_score: 0.8851748684617765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:16<00:25,  8.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD:3, epoch: 1, accuracy_score: 0.8916744042092232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [00:24<00:16,  8.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD:3, epoch: 2, accuracy_score: 0.8960074280408542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [00:32<00:08,  8.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD:3, epoch: 3, accuracy_score: 0.8876508820798514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:41<00:00,  8.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD:3, epoch: 4, accuracy_score: 0.8882698854843701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:08<00:33,  8.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD:4, epoch: 0, accuracy_score: 0.8644382544103992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:16<00:24,  8.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD:4, epoch: 1, accuracy_score: 0.8687712782420304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [00:25<00:17,  8.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD:4, epoch: 2, accuracy_score: 0.8771278242030331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [00:34<00:08,  8.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD:4, epoch: 3, accuracy_score: 0.8774373259052924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:41<00:00,  8.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD:4, epoch: 4, accuracy_score: 0.8783658310120706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('Load embeddings')\n",
    "embedding_matrix = create_embedding_matrix(tokenizer.word_index, embedding_dict=word_vectors, dim=300)\n",
    "\n",
    "\n",
    "for fold in range(5):\n",
    "    # STEP 2: cross validation\n",
    "    train_df = traindf[traindf.kfold != fold].reset_index(drop=True)\n",
    "    valid_df = traindf[traindf.kfold == fold].reset_index(drop=True)\n",
    "    \n",
    "    # STEP 3: pad sequence\n",
    "    xtrain = tokenizer.texts_to_sequences(train_df.review.values)\n",
    "    xtest = tokenizer.texts_to_sequences(valid_df.review.values)\n",
    "    \n",
    "    # zero padding\n",
    "    xtrain = tf.keras.preprocessing.sequence.pad_sequences(xtrain, maxlen=MAX_LEN)\n",
    "    xtest = tf.keras.preprocessing.sequence.pad_sequences(xtest, maxlen=MAX_LEN)\n",
    "    \n",
    "    # STEP 4: initialize dataset class for training\n",
    "    train_dataset = IMDBDataset(reviews=xtrain, targets=train_df['sentiment'].values)\n",
    "    \n",
    "    # STEP 5: Load dataset to Pytorch DataLoader\n",
    "    # after we have train_dataset, we create a torch dataloader to load train_dataset class based on specified batch_size\n",
    "    train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size = TRAIN_BATCH_SIZE, num_workers=2)\n",
    "    # initialize dataset class for validation\n",
    "    valid_dataset = IMDBDataset(reviews=xtest, targets=valid_df['sentiment'].values)\n",
    "    valid_data_loader = torch.utils.data.DataLoader(valid_dataset, batch_size = VALID_BATCH_SIZE, num_workers=1)\n",
    "    \n",
    "    # STEP 6: Running \n",
    "    device = torch.device('cuda')\n",
    "    # feed embedding matrix to lstm\n",
    "    model_fasttext = LSTM(embedding_matrix)\n",
    "    # set model to cuda device\n",
    "    model_fasttext.to(device)\n",
    "    # initialize Adam optimizer\n",
    "    optimizer = torch.optim.Adam(model_fasttext.parameters(), lr=1e-3)\n",
    "    \n",
    "    print('training model')\n",
    "\n",
    "    for epoch in tqdm(range(EPOCHS)):\n",
    "        # train one epoch\n",
    "        train(train_data_loader, model_fasttext, optimizer, device)\n",
    "        # validate\n",
    "        outputs, targets = evaluate(valid_data_loader, model_fasttext, device)\n",
    "        # threshold\n",
    "        outputs = np.array(outputs) >= 0.5\n",
    "        # calculate accuracy\n",
    "        accuracy = metrics.accuracy_score(targets, outputs)\n",
    "        print(f'FOLD:{fold}, epoch: {epoch}, accuracy_score: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on test set testdf and testy\n",
    "xtest = tokenizer.texts_to_sequences(testdf.review.values)\n",
    "xtest = tf.keras.preprocessing.sequence.pad_sequences(xtest, maxlen=MAX_LEN)\n",
    "test_dataset = IMDBDataset(reviews=xtest, targets=testy)\n",
    "test_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size = VALID_BATCH_SIZE, num_workers=1)\n",
    "outputs, targets = evaluate(test_data_loader, model_fasttext, device)\n",
    "outputs = np.array(outputs) >= 0.5\n",
    "accuracy = metrics.accuracy_score(targets, outputs)\n",
    "print(f'accuracy_score: {accuracy}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
